{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hi there \ud83d\udd96! \u00b6 My name is Alejandro Piad Morffis. Here are some things about me: I live in Havana, Cuba. I'm currently finishing a PhD in Computer Science, part-time in Alicante, Spain. In my free time, I also enjoy coding (mostly in Python), playing video games (sadly not much lately), and writing . My two passions are teaching and doing research . I teach Programming, Compilers, AI, and a bunch of other stuff at the University of Havana. I also do research there, mostly on how to use artificial intelligence to better understand language, and on the democratization of machine learning tools. You can find me online on Twitter , LinkedIn , Reddit , Telegram and Youtube . The best way to contact me is to mention @AlejandroPiad on Twitter. I follow very few people (only those with whom I interact frequently) but I try to reply whenever someone asks me to. And these are the values I stand for: I believe that people are generally good, and if given the chance, they will show the better parts of themselves. I believe people should have the chance to speak their minds, without fear to be silenced or hated for it, even if they are wrong. And they should have the option to honestly recognize their mistakes, learn from them, and be forgiven. I do not tolerate racism or discrimination of any kind, towards me or others around me, and I work very hard to apply those same standards to myself. I'm dedicated to education because I think that access to high-quality, unbiased, and free education is one of the best gifts anyone can receive, and one of the easiest ways to make people more reasonable and tolerant.","title":"\ud83d\udd96 Hello World!"},{"location":"#hi-there","text":"My name is Alejandro Piad Morffis. Here are some things about me: I live in Havana, Cuba. I'm currently finishing a PhD in Computer Science, part-time in Alicante, Spain. In my free time, I also enjoy coding (mostly in Python), playing video games (sadly not much lately), and writing . My two passions are teaching and doing research . I teach Programming, Compilers, AI, and a bunch of other stuff at the University of Havana. I also do research there, mostly on how to use artificial intelligence to better understand language, and on the democratization of machine learning tools. You can find me online on Twitter , LinkedIn , Reddit , Telegram and Youtube . The best way to contact me is to mention @AlejandroPiad on Twitter. I follow very few people (only those with whom I interact frequently) but I try to reply whenever someone asks me to. And these are the values I stand for: I believe that people are generally good, and if given the chance, they will show the better parts of themselves. I believe people should have the chance to speak their minds, without fear to be silenced or hated for it, even if they are wrong. And they should have the option to honestly recognize their mistakes, learn from them, and be forgiven. I do not tolerate racism or discrimination of any kind, towards me or others around me, and I work very hard to apply those same standards to myself. I'm dedicated to education because I think that access to high-quality, unbiased, and free education is one of the best gifts anyone can receive, and one of the easiest ways to make people more reasonable and tolerant.","title":"Hi there \ud83d\udd96!"},{"location":"about/projects/","text":"Here are some of the most interesting projects I'm working on: AutoGOAL \u00b6 \ud83d\udde8\ufe0f Ask me on Twitter. AutoGOAL is a Python framework for Automated Machine Learning that I and a few other colleagues are building. It's the main research result of my wife's Ph.D. , and it is a team project in which I'm a proud contributor and evangelist. Auditorium \u00b6 \ud83d\udde8\ufe0f Ask me on Twitter. Auditorium is my attempt to bring together Python with HTML for interactive slideshows. It is based on reveal.js and allows creating a slideshow with pure Python code, including some interactive stuff like rendering graphs on the fly and running animations. Illiterate \u00b6 \ud83d\udde8\ufe0f Ask me on Twitter. Illiterate is my take at the dilemma of code comments vs documentation. The idea stems from my love for the Literate Programming paradigm with a pragmatic twist that doesn't require any external tooling but rather relies on discipline and conventions.","title":"\ud83d\udcbb Projects"},{"location":"about/projects/#autogoal","text":"\ud83d\udde8\ufe0f Ask me on Twitter. AutoGOAL is a Python framework for Automated Machine Learning that I and a few other colleagues are building. It's the main research result of my wife's Ph.D. , and it is a team project in which I'm a proud contributor and evangelist.","title":"AutoGOAL"},{"location":"about/projects/#auditorium","text":"\ud83d\udde8\ufe0f Ask me on Twitter. Auditorium is my attempt to bring together Python with HTML for interactive slideshows. It is based on reveal.js and allows creating a slideshow with pure Python code, including some interactive stuff like rendering graphs on the fly and running animations.","title":"Auditorium"},{"location":"about/projects/#illiterate","text":"\ud83d\udde8\ufe0f Ask me on Twitter. Illiterate is my take at the dilemma of code comments vs documentation. The idea stems from my love for the Literate Programming paradigm with a pragmatic twist that doesn't require any external tooling but rather relies on discipline and conventions.","title":"Illiterate"},{"location":"about/research/","text":"","title":"\u2697\ufe0f Research"},{"location":"about/teaching/","text":"","title":"\ud83c\udf93 Teaching"},{"location":"about/writing/","text":"Writing is another of my passions. I'm not very good at it, but nevertheless, I really enjoy pouring my thoughts into a more durable form than the electric brain waves they're born from. Some of the most serious things I've written can be found in the Essays section . These are short-to-medium length takes on some interesting subject. It can be some sort of philosophical topic, or just some entertaining issue for me. The main characteristic of these \"essays\" is that they reflect a part of what I think, and thus I try to keep them up-to-date. Then there is the Guides section which collects short technical... well... guides, on topics that I think are interesting. Right now I'm working on one on technical writing. Finally, I recently started journaling, and I'm collecting my daily (or whenever I feel like) mussings on whatever I happen to be focusing on that day. This is the least polished material of all, stored here mainly for my own recall in the near future. Hopefully, some of those ideas will grow and become worthy of their own space at some point, either as projects, research papers, essays, or guides.","title":"\u270f\ufe0f Writing"},{"location":"essays/","text":"This is a collection of some things I've written over the years. Instead of blog posts, I consider them kind of short essays. These are varied in format and content, some are more technical, some are more philosophical, others a bit more pragmatic. I honestly don't like to write tutorial-like articles. I've tried a bunch of times and I've failed (though I might try again in the future). For now, I think there are enough amazing people out there doing it much better than I. Hence, these short essays are not about specific technologies or how-to guides. I try to select topics I'm passionate about and that I think have a chance to remain relevant for a longer time. If there is a particular topic you think I might have something interesting to say about, feel free to contact me on Twitter .","title":"Prologue"},{"location":"essays/academia-oss/","text":"What Academia can learn from Open Source \u00b6 I'm an academic. I love doing research and writing papers. What I don't love is playing the publishing game and waste my time micro-managing all these bureaucratic aspects of academia. I also love open-source software, and while the FOSS community is far from perfect, there are some ideas I think Academia could borrow that would make it more inclusive for everyone and more useful for society. \u26a0\ufe0f This is a rant about some things I think are wrong in Academia and some ideas about how to improve this situation. I mostly focus on Artificial Intelligence because that's my field, but I think most of these ideas apply everywhere. I'm not trying to discredit or criticize any individual or organization, but rather raise some questions that I think all of us scientists, as a community, should attend. I declare myself as guilty of all the sins I describe. \ud83d\uddde\ufe0f The setup \u00b6 If you've ever tried Academia you have surely been in this situation. You come up with a good idea, do some experiments, write a paper about it and... that's when the real work starts. Whether you send that paper to a conference or a journal, you'll get 2 to 5 reviewers to critic your paper, ask you for improvements and decide if your work is good enough for publication. If it's a conference, you'll usually either get accepted or rejected, but if it's a journal, you might get a second chance to improve and resubmit. This process is called peer review , and it's one of the fundamental pillars of Science. Don't get me wrong, peer review is extremely important. You see, Science is a social process. Yes, you can follow the scientific method and come up with a Frankenstein monster all by yourself on a private island, and you would be doing science (without capital \"s\"). It is only when those results are scrutinized, retested, and confirmed by additional researchers, that they become part of the continuous and incremental body of accumulated knowledge that we call Science. Peer review is a fundamental part of this process because it ensures that you are not deluding yourself into believing what you want to believe. It also guarantees we all follow the same high standards of openness, honesty, and goodwill. However, problems arise when the means become an end in itself. Since peer review is such an important concept in Science, we have built all our social scientific processes around it. We set deadlines, ratings, whole systems to formalize and organize what peer review means. We have double-blind and single-blind peer review to guarantee that authors and reviewers don't take revenge on each other. We have evaluation forms and protocols, and we have workshops and workshops about peer review. And yet, time and time again, experiments have shown that reviews are significantly inconsistent. If you randomly redistribute the papers on a top AI conference, a large part of the accepted papers get rejected, and vice-versa. However, I do not take this as evidence that scientists are lousy reviewers. Not even close. Scientists are pretty good at being objectively critical of other's and our work, we do that every single day! I think the problem lies in the system and the incentives built around it, mostly for the benefit of the big players in the Academic world, the publishers. \ud83e\udd15 The symptoms \u00b6 Every time you take a metric and turn it into an objective, it ceases to be a useful metric. This has happened in Science with the concept of publishing a paper . Publishing a paper is the main mechanism for socializing research. A research paper usually describes some scientific hypotheses in as clear terms as possible, a protocol to test (i.e., falsify) those hypotheses, and an honest and critical discussion of results and their implications. By reading a paper, fellow scientists can come up with additional hypotheses or ideas, and build on top of previous work. And every time you use someone else's ideas as part of your own, you are supposed to include a citation. This is what Newton was referring to when he said he had \"stand on the shoulder of giants\". In time, the most significant scientific discoveries should get a large number of citations, because everyone building on top of your ideas would cite you. Hence, a large number of citations is seen as a sign of scientific achievement, and that is often taken as the One Metric of Academic Success . See the problem here? Once citations become a distinction mark, everyone tries to maximize them. A lot of strategies begin to arise, like publishing lots of low-effort papers instead of fewer and better ones, and working only on the most fashionable topics. Since to get cited you have to get published first, publishers become the gatekeepers. A feedback loop starts to build in which publishers try to be as exclusive as possible to attract better papers, since more citations imply more readers which implies more subscriptions; and authors try to aim for the most exclusive publishers since, otherwise, they won't get enough citations. In this dynamic, two very harmful things start to happen. \ud83c\udd70\ufe0f First, scientists spend a lot of effort and money, very often public money, on research that never gets published because of the massive competition. Ironically, once that research made with public money is published, is often put behind a subscription paywall, which most universities and institutions subscribe to. So taxpayers end up paying for research twice, once when done by Alice and again when Bob wants to read Alice's paper. Isn't that crazy enough? \ud83c\udd71\ufe0f The second issue is more subtle but far more harmful. In this process of out-competing each other for citations, we forgot what's important about Science. It's a social process designed to improve human life by solving humanity's most pressing problems. But this competition, far from what free-market ideologists could believe, only serves to undermine the very purpose of Science: \ud83d\udc4e The most fashionable topics get the most attention, and those are often not correlated with the need of the many. \ud83d\udc4e Also, scientists are not born, they are educated. If competition is so fierce that junior researchers don't get a break, we end up losing the best minds before they get a chance to shine. \ud83d\udc4e And finally, this constant competition for citations discourages any kind of self-critic research, any analysis of negative results, and any replication study, because no one will cite you for saying \" yeah, I retested this, and it does seem to work as they originally said... \". This discussion started with peer review, and how the whole academic publishing is built around this concept. Now is the time to criticize it. Since scientists are forced to compete for attention, we have turned peer review from the supportive and self-healing process it should be into the most unpleasant part of doing research. To be fair, not all reviewers are nasty, and when we do, I'm arguing is more often than not because we are forced by the system. \u2b50 The new paradigm \u00b6 I believe the root of the problem in this picture should be clear by now. \u26a0\ufe0f The incentives for scientists are not aligned with the purpose of Science. So, how do we realign the incentives of scientists with the original purpose of Science, and make it better for everyone? Honestly, I don't know. But I think we can take some ideas from the FOSS community to at least foster some good practices which I believe might put us on the right track. The idea starts with embracing Openness in the whole process of scientific discovery and innovation. This is not my original idea, of course, there are some commonly shared principles of \"open science\" in the academic community. This is one possible way to express them: Open Methodology : Document the application of methods and the entire process behind them as far as practicable and relevant. Open Source : Use open source technology (software and hardware) and open your own technologies. Open Data : Make the data freely and easily available. Open Access : Publish openly and make publications usable and accessible to everyone. Open Peer Review : Provide peer review in an open and public forum. Open Educational Resources : Use free and open materials for education and in university teaching. In this form, these principles are quite abstract, and there are many ways in which they could be implemented. There are plenty of degrees of \"open science\" like publishing in open access journals managed by non-profit organizations, publishing pre-prints before submitting to \"traditional\" journals, and all the good practices around making data and protocols publicly available. I want to focus on some key ideas I think could be fruitful to try, without implying that this is the absolute solution to this problem, but rather a small part of a much larger paradigm shift that Science has to undertake. \u2699\ufe0f The practices \u00b6 These are my proposals. Most of them relate specifically to the peer-review process because, as I said before, this process is a pillar of the scientific process, but also because I think this is the one place where we as a community can innovate the most, without requiring government grants or changing the way bureaucratic institutions work. The peer-review process is at the base of the entire scientific process and any major change in its functioning could have a massive impact up the chain. 1\ufe0f\u20e3 Public reviews \u00b6 Let's start by acknowledging that single- and double-blind reviews are more harmful than helpful. These measures are supposed to shield reviewers and authors from future retaliation and disallow any form of favouritism, which should make the review process more just and honest. In practice, they shield reviewers from criticism and make the whole review process less transparent. I propose to turn this concept around and make all reviews completely public. We have to trust we are all reasonable individuals and professional scientists, who should be able to provide objective judgment without favouritism. But if we don't, then our reviews themselves are public, and subject to review and criticism. \ud83d\udc49 This is very easy to implement with any workflow that allows posting comments on a public forum. Note that I don't necessarily mean that anyone can review (this is discussed further down) but even if only specific \"official\" reviewers are assigned to a paper, their comments and their identity and credentials should be public. 2\ufe0f\u20e3 Continuous peer-review \u00b6 This idea ties in with the previous point. Currently, almost all peer-review (that I'm aware of) happens in the context of some specific conference or journal. What I'm proposing here is to detach the peer-review process from any journal or conference and make it instead integral to the paper. Every paper would carry around the Internet with all its reviews, and if rejected at some previous point, a future conference or journal editor would have access to the full history of reviews and changes to reconsider the paper for \"mainstream\" publication (we'll talk more about what this means later). \ud83d\udc49 I can see this happening similar to how issues are handled in Github. You publish a paper, and potential reviewers would open \"issues\" against it, one for each important thing to address. Issues would be discussed and worked on in public and there would a history of every change introduced into the paper with links to which issues are being fixed. Since no paper is perfect, conferences and journal editors should not aim for publishing issue-free papers, but rather papers that show a healthy list of open and closed issues and demonstrable usefulness in their current state. A healthy list of open and closed issues would be an indication of a solid paper, the same way as for software. 3\ufe0f\u20e3 Encouraging reviews \u00b6 The next problem I want to tackle consists of how to kick-start the reviewing process. Once we detach reviews from specific conferences or journals, how can we guarantee everyone has access to good reviews? For sure, rockstar scientists will get thousands of reviews but what about junior researchers who are just starting? One idea is to see reviewing as an integral part of the scientific career. Researchers should be evaluated also in terms of how much value they put back to the community, and one way senior scientists can contribute is to review junior scientists. We should be proud to put in our CVs how many reviews we have given. And good reviews, which are in turn evaluated positively by the author and other reviewers, should count towards one's scientific output. \ud83d\udc49 Scientists would get a \"badge\" with the number reviews they have given, and display it on their homepage, their LinkedIn or ResearchGate profile, etc. This badge would link to some online list that links back to all reviews. This could be maybe hosted in our ORCID profiles or any similar non-profit initiative. Also, senior researchers are part of a community, and are often connected with like-minded individuals in other institutions and countries. It should be part of their work to look up for each other's students and junior researchers. And yes, someone will say \" but then you can give a good review to my students if I give a good review to yours \". Again, this is why all review is, first and foremost, public in nature. 4\ufe0f\u20e3 Qualitative evaluations \u00b6 Now let's move on to specific review formats. Too often I see very long lists of checkboxes and 1-5 ratings, etc. I believe there is value in having a structured evaluation template, to make sure we more or less agree on what are the core issues we should care about. But going to the extreme of having 10 different ratings for a paper is insane! What is the difference between 6 and 7? When putting reviewers under the pressure of giving numerical scores, we are asking them to unconsciously introduce all the biases they have about that particular problem or field or approach or author. There is simply no objective way to numerically compare two different papers. A good research paper needs to have a solid methodology (correctly apply the principles of science as it is common practice in that field), provide relevant results and conclusions (either positive or negative), be feasible to reproduce by independent researchers, and have a clear presentation. Either the paper is good enough to be considered publishable, if all these aspects are covered, or it isn't. That's it. \ud83d\udc49 I prefer a simple evaluation form that asks: \" Is this aspect of the paper up to the scientific standard? , and a piece of free-form text for you to explain what is lacking in each aspect. Methodology [x] \ud83d\udc4d [ ] \ud83d\udc4e Results [x] \ud83d\udc4d [ ] \ud83d\udc4e Reproducibility [ ] \ud83d\udc4d [x] \ud83d\udc4e Presentation [ ] \ud83d\udc4d [x] \ud83d\udc4e A specific conference or journal might want to evaluate the potential impact or significance of a paper before accepting it for publishing. But impact or significance is not what Science is about. There are however legitimate cases where impact or significance is important. If you have to allocate a restricted pool of resources (e.g., grant money) of course you want to evaluate impact. Yet, I argue this is not part of the peer-review process, but a posterior analysis that each institution or publisher should do based on their specific criteria. Peer-review should be a process by which the scientific community as a whole evaluates that some research is sound science, irrespective of idiosyncrasies. 5\ufe0f\u20e3 Self-publishing \u00b6 Now that the peer-review process is completely detached from the \"mainstream\" publishing industry, who decides when is a paper ready to be published? Well, of course, the authors! It is up to the authors to determine that, given all the feedback received, they consider their work is production-ready. \ud83d\udc49 All papers would be published first in draft-mode, perhaps even before being completely written. During the draft phase, you collect all the feedback you can from peer reviews and work on the issues you consider more relevant. When you feel it is good enough (possibly because most of the recent reviews are favourable) you hit that Publish button and create a release. If some errors appear, later on, you fix them and publish another release. \ud83d\udc49 What becomes citable then? Easy, each release of each paper gets a unique DOI that will forever point to that exact version, together with all its metadata and reviews. If I cite something yours and criticize it, and you later fix it, that's OK. My critic is still valid because it points to a previous version that is indexable, and the fact that you fixed it only speaks higher of you! But wait, won't authors publish a lot of low-effort papers to engross their CVs? Well, maybe someone will but, who cares? We started by saying that, intuitively, citations should be a good measure of scientific quality. This is still true in this format. If I release a bunch of crappy papers, no one will cite them. And also, who thinks CVs are useful? Anyone trying to evaluate me as a researcher would not look into some list of titles and numbers I pasted in a Word document. They would go to my researcher profile and see my most significant work, the reviews it has received, and how my whole research process works! This doesn't mean that I get to decide my work is relevant, however. This just means I get to decide my work is ready to be consumed by the scientific community. The community will still judge my work's relevance by citing it, criticizing it, and in two more ways I left for the end. 6\ufe0f\u20e3 Conferences for networking \u00b6 Now that all papers are being published by their authors, what's the purpose of scientific conferences? We can now recover their original purpose. Conferences were created as a medium to get like-minded scientists together to share their experiences and to discuss the most relevant problems of their field. But as conferences became more and more a mainstream path for getting published, their organization has become more and more about managing the peer-review process. Now that peer-review is detached from conferences, their organizers are free to focus on scooping what are the most interesting topics and the most significant results in those topics and invite those speakers they believe will the bring the biggest value. Which papers get to be presented? I think we could deal with that in two ways: \ud83d\udc49 As an organizer of a thematic conference, I would spend half of the year looking around for interesting papers to invite their authors. They would still pay for their accommodations (or their institutions would) and they would come to enjoy what's best in every conference, the networking. \ud83d\udc49 I could also open a call for papers, as usual, but authors would submit papers that are already reviewed and released. My role would be to decide, based primarily on thematic fit, what I think is more relevant for my community. \ud83d\udc49 There is even no need to attach participation to a published paper. Authors could simply submit \"talks\", possibly backed by one or more papers that support their submission, as it is already common in some conferences. This would completely reshape what conferences are for (at least in my field). There is no reason why we should wait to the top conferences of the year to be able to read papers. We would go to conferences for the chance to talk with the researchers we admire about their work. And someone asks, but how would conferences compete if they have no publication rights? Well, I argue this would be very good. Conference organizers would have to compete on the grounds of providing a better environment for networking, interesting events, nice amenities, but no one would have a monopoly on the knowledge itself. There is even no reason why the same talk cannot be presented in more than one conference if enough people are willing to listen. 7\ufe0f\u20e3 Journals for socializing \u00b6 And finally we come round to the original culprit, the infamous research journal. Now that papers are published openly, what are journals good for? Well, what they were originally designed for, socializing research! Journals were created as a means for academic societies to collect the most relevant research in a given community and publish it for a larger audience. Then commercial publishers arrived and turned science into a business, and journals became paywalled gatekeepers of knowledge, that require original research often paid with public money that they resell again for public money. The largest academic publishers often state they have costs to cover, but there is plenty of evidence that they make a significant profit. And that's OK, but if I as a journal editor want to make a profit, I'm gonna have to innovate. \ud83d\udc49 Like conferences, I could scoop around and feature the most interesting papers in some thematic issues, maybe ask the authors to give some new comments on them, prepare explainer videos, add links, and put some effort into turning those \"raw\" papers into beautifully typeset pages. \ud83d\udc49 SOTA reviews would be a nice fit for journals as well. These are not original research papers, but they often provide a lot of value by analyzing a bunch of papers and giving advice on common trends or highlighting interesting lines for future research. \ud83d\udc49 I would also have editorial articles specifically written for an issue that could summarize in layman terms about a particular subject, to introduce it to a larger audience. I would even pay scientists that are good communicators for this work. Journals would have to compete on the grounds of being good at selecting topics and papers to socialize, and provide some additional editorial value. In any case, original research papers would be owned only by their authors and would be published always with some public license (e.g., Creative Commons). This would ensure that Science belongs to the ones who ultimately pay for it, that is, society. \ud83d\udcbb The system \u00b6 Putting all these ideas together in a functioning system will require a lot of work. From the infrastructure point of view, I envision something aking Github, a repository of open access papers with builtin comments, reviewing, and social features. Ideally, it would also have a web UI for editing, similar to Overleaf and, of course, fully integrated with Git. I understand this might not be the best solution for academic communities that are not very closely related to software, i.e., social sciences, mostly because it could pose a significant learning curve for their members and become more of a hinder than a help. From the social point of view, kick-starting such a system would require a massive community effort. And not because of the infrastructure cost, that's the minor issue. I think the largest obstacle for this kind of paradigm shift is that a large part of the community would have to move away simultaneously from journals and conferences as the main publication channels. Otherwise, the few that start the effort will be completely disconnected from the rest of the community. I can see this happening as an effort from, say, the AI community, or any other technically-savvy collective. Tomorrow morning, all the senior scientists that publish in ICLR, ICML, NeurIPs, and ACL, could suddenly decide they want to go fully open. It would require the conference organizers to support the initiative as well. Instead of opening a call for papers, the conferences could decide that they would open a call for submissions, which should peer-reviewed and published in this new format. Some non-profit organization could be formed from within the community to provide the infrastructure. Since there will be some operative costs, this platform would require some payment, but it would be very small compared to publishing fees in most major open access journals. Also, some of the big players in the industry could support this initiative by providing hosting and infrastructure for free. This would be a big PR boost to these companies. \ud83d\udcca The metrics \u00b6 We started this discussion by saying that an intuitively good metric to estimate scientific impact, i.e., citations, had become an objective and thus lost their entire meaning. But citations are not an inherently bad metric, it's just when we use citations as the one quantitative metric to compare individual researchers that we miss the entire point. Likewise, the fact that we self-publish all our papers doesn't mean that being featured in a major conference or journal is worthless. On the contrary, when everything we publish is open, being featured in a mainstream publication becomes an even better measure of impact, because it is no longer tied to my financial capacity or any other unfair advantage I might enjoy in the community. This would be very good for Third World researchers, who produce valuable science, but are often cutoff from mainstream publication for reasons completely unrelated to the quality of the research. If we can restructure the incentives and processes of Science such that they are aligned with the purpose of Science as primarily a means to improve human life, everything else would fall into place. Once researchers are free from predatory publishing practices, meaningless numerical statistics and unhealthy competition, I believe we will all focus what we love most, doing sound research for the good of mankind. Then, all those metrics that are used today will regain their meaning. Being invited to a top-tier conference would mean that what your community wants to hear from you. Being featured in a top-tier journal would mean that some editors consider your work is high-quality. And being cited often would mean that your research is producing real impact, that you are becoming a giant on whose shoulders others can stand. \ud83d\udde8\ufe0f This is a topic of which I'm very passionate about, and I want to hear your feedback. If you want to discuss this topic with me, give me a shout in Twitter . \ud83d\udeab If you find some typo, error or you have any suggestion to help me improve this page, you can suggest and edit or open an issue in Github .","title":"Academia & OSS"},{"location":"essays/academia-oss/#what-academia-can-learn-from-open-source","text":"I'm an academic. I love doing research and writing papers. What I don't love is playing the publishing game and waste my time micro-managing all these bureaucratic aspects of academia. I also love open-source software, and while the FOSS community is far from perfect, there are some ideas I think Academia could borrow that would make it more inclusive for everyone and more useful for society. \u26a0\ufe0f This is a rant about some things I think are wrong in Academia and some ideas about how to improve this situation. I mostly focus on Artificial Intelligence because that's my field, but I think most of these ideas apply everywhere. I'm not trying to discredit or criticize any individual or organization, but rather raise some questions that I think all of us scientists, as a community, should attend. I declare myself as guilty of all the sins I describe.","title":"What Academia can learn from Open Source"},{"location":"essays/academia-oss/#the-setup","text":"If you've ever tried Academia you have surely been in this situation. You come up with a good idea, do some experiments, write a paper about it and... that's when the real work starts. Whether you send that paper to a conference or a journal, you'll get 2 to 5 reviewers to critic your paper, ask you for improvements and decide if your work is good enough for publication. If it's a conference, you'll usually either get accepted or rejected, but if it's a journal, you might get a second chance to improve and resubmit. This process is called peer review , and it's one of the fundamental pillars of Science. Don't get me wrong, peer review is extremely important. You see, Science is a social process. Yes, you can follow the scientific method and come up with a Frankenstein monster all by yourself on a private island, and you would be doing science (without capital \"s\"). It is only when those results are scrutinized, retested, and confirmed by additional researchers, that they become part of the continuous and incremental body of accumulated knowledge that we call Science. Peer review is a fundamental part of this process because it ensures that you are not deluding yourself into believing what you want to believe. It also guarantees we all follow the same high standards of openness, honesty, and goodwill. However, problems arise when the means become an end in itself. Since peer review is such an important concept in Science, we have built all our social scientific processes around it. We set deadlines, ratings, whole systems to formalize and organize what peer review means. We have double-blind and single-blind peer review to guarantee that authors and reviewers don't take revenge on each other. We have evaluation forms and protocols, and we have workshops and workshops about peer review. And yet, time and time again, experiments have shown that reviews are significantly inconsistent. If you randomly redistribute the papers on a top AI conference, a large part of the accepted papers get rejected, and vice-versa. However, I do not take this as evidence that scientists are lousy reviewers. Not even close. Scientists are pretty good at being objectively critical of other's and our work, we do that every single day! I think the problem lies in the system and the incentives built around it, mostly for the benefit of the big players in the Academic world, the publishers.","title":"\ud83d\uddde\ufe0f The setup"},{"location":"essays/academia-oss/#the-symptoms","text":"Every time you take a metric and turn it into an objective, it ceases to be a useful metric. This has happened in Science with the concept of publishing a paper . Publishing a paper is the main mechanism for socializing research. A research paper usually describes some scientific hypotheses in as clear terms as possible, a protocol to test (i.e., falsify) those hypotheses, and an honest and critical discussion of results and their implications. By reading a paper, fellow scientists can come up with additional hypotheses or ideas, and build on top of previous work. And every time you use someone else's ideas as part of your own, you are supposed to include a citation. This is what Newton was referring to when he said he had \"stand on the shoulder of giants\". In time, the most significant scientific discoveries should get a large number of citations, because everyone building on top of your ideas would cite you. Hence, a large number of citations is seen as a sign of scientific achievement, and that is often taken as the One Metric of Academic Success . See the problem here? Once citations become a distinction mark, everyone tries to maximize them. A lot of strategies begin to arise, like publishing lots of low-effort papers instead of fewer and better ones, and working only on the most fashionable topics. Since to get cited you have to get published first, publishers become the gatekeepers. A feedback loop starts to build in which publishers try to be as exclusive as possible to attract better papers, since more citations imply more readers which implies more subscriptions; and authors try to aim for the most exclusive publishers since, otherwise, they won't get enough citations. In this dynamic, two very harmful things start to happen. \ud83c\udd70\ufe0f First, scientists spend a lot of effort and money, very often public money, on research that never gets published because of the massive competition. Ironically, once that research made with public money is published, is often put behind a subscription paywall, which most universities and institutions subscribe to. So taxpayers end up paying for research twice, once when done by Alice and again when Bob wants to read Alice's paper. Isn't that crazy enough? \ud83c\udd71\ufe0f The second issue is more subtle but far more harmful. In this process of out-competing each other for citations, we forgot what's important about Science. It's a social process designed to improve human life by solving humanity's most pressing problems. But this competition, far from what free-market ideologists could believe, only serves to undermine the very purpose of Science: \ud83d\udc4e The most fashionable topics get the most attention, and those are often not correlated with the need of the many. \ud83d\udc4e Also, scientists are not born, they are educated. If competition is so fierce that junior researchers don't get a break, we end up losing the best minds before they get a chance to shine. \ud83d\udc4e And finally, this constant competition for citations discourages any kind of self-critic research, any analysis of negative results, and any replication study, because no one will cite you for saying \" yeah, I retested this, and it does seem to work as they originally said... \". This discussion started with peer review, and how the whole academic publishing is built around this concept. Now is the time to criticize it. Since scientists are forced to compete for attention, we have turned peer review from the supportive and self-healing process it should be into the most unpleasant part of doing research. To be fair, not all reviewers are nasty, and when we do, I'm arguing is more often than not because we are forced by the system.","title":"\ud83e\udd15 The symptoms"},{"location":"essays/academia-oss/#the-new-paradigm","text":"I believe the root of the problem in this picture should be clear by now. \u26a0\ufe0f The incentives for scientists are not aligned with the purpose of Science. So, how do we realign the incentives of scientists with the original purpose of Science, and make it better for everyone? Honestly, I don't know. But I think we can take some ideas from the FOSS community to at least foster some good practices which I believe might put us on the right track. The idea starts with embracing Openness in the whole process of scientific discovery and innovation. This is not my original idea, of course, there are some commonly shared principles of \"open science\" in the academic community. This is one possible way to express them: Open Methodology : Document the application of methods and the entire process behind them as far as practicable and relevant. Open Source : Use open source technology (software and hardware) and open your own technologies. Open Data : Make the data freely and easily available. Open Access : Publish openly and make publications usable and accessible to everyone. Open Peer Review : Provide peer review in an open and public forum. Open Educational Resources : Use free and open materials for education and in university teaching. In this form, these principles are quite abstract, and there are many ways in which they could be implemented. There are plenty of degrees of \"open science\" like publishing in open access journals managed by non-profit organizations, publishing pre-prints before submitting to \"traditional\" journals, and all the good practices around making data and protocols publicly available. I want to focus on some key ideas I think could be fruitful to try, without implying that this is the absolute solution to this problem, but rather a small part of a much larger paradigm shift that Science has to undertake.","title":"\u2b50 The new paradigm"},{"location":"essays/academia-oss/#the-practices","text":"These are my proposals. Most of them relate specifically to the peer-review process because, as I said before, this process is a pillar of the scientific process, but also because I think this is the one place where we as a community can innovate the most, without requiring government grants or changing the way bureaucratic institutions work. The peer-review process is at the base of the entire scientific process and any major change in its functioning could have a massive impact up the chain.","title":"\u2699\ufe0f The practices"},{"location":"essays/academia-oss/#1-public-reviews","text":"Let's start by acknowledging that single- and double-blind reviews are more harmful than helpful. These measures are supposed to shield reviewers and authors from future retaliation and disallow any form of favouritism, which should make the review process more just and honest. In practice, they shield reviewers from criticism and make the whole review process less transparent. I propose to turn this concept around and make all reviews completely public. We have to trust we are all reasonable individuals and professional scientists, who should be able to provide objective judgment without favouritism. But if we don't, then our reviews themselves are public, and subject to review and criticism. \ud83d\udc49 This is very easy to implement with any workflow that allows posting comments on a public forum. Note that I don't necessarily mean that anyone can review (this is discussed further down) but even if only specific \"official\" reviewers are assigned to a paper, their comments and their identity and credentials should be public.","title":"1\ufe0f\u20e3 Public reviews"},{"location":"essays/academia-oss/#2-continuous-peer-review","text":"This idea ties in with the previous point. Currently, almost all peer-review (that I'm aware of) happens in the context of some specific conference or journal. What I'm proposing here is to detach the peer-review process from any journal or conference and make it instead integral to the paper. Every paper would carry around the Internet with all its reviews, and if rejected at some previous point, a future conference or journal editor would have access to the full history of reviews and changes to reconsider the paper for \"mainstream\" publication (we'll talk more about what this means later). \ud83d\udc49 I can see this happening similar to how issues are handled in Github. You publish a paper, and potential reviewers would open \"issues\" against it, one for each important thing to address. Issues would be discussed and worked on in public and there would a history of every change introduced into the paper with links to which issues are being fixed. Since no paper is perfect, conferences and journal editors should not aim for publishing issue-free papers, but rather papers that show a healthy list of open and closed issues and demonstrable usefulness in their current state. A healthy list of open and closed issues would be an indication of a solid paper, the same way as for software.","title":"2\ufe0f\u20e3 Continuous peer-review"},{"location":"essays/academia-oss/#3-encouraging-reviews","text":"The next problem I want to tackle consists of how to kick-start the reviewing process. Once we detach reviews from specific conferences or journals, how can we guarantee everyone has access to good reviews? For sure, rockstar scientists will get thousands of reviews but what about junior researchers who are just starting? One idea is to see reviewing as an integral part of the scientific career. Researchers should be evaluated also in terms of how much value they put back to the community, and one way senior scientists can contribute is to review junior scientists. We should be proud to put in our CVs how many reviews we have given. And good reviews, which are in turn evaluated positively by the author and other reviewers, should count towards one's scientific output. \ud83d\udc49 Scientists would get a \"badge\" with the number reviews they have given, and display it on their homepage, their LinkedIn or ResearchGate profile, etc. This badge would link to some online list that links back to all reviews. This could be maybe hosted in our ORCID profiles or any similar non-profit initiative. Also, senior researchers are part of a community, and are often connected with like-minded individuals in other institutions and countries. It should be part of their work to look up for each other's students and junior researchers. And yes, someone will say \" but then you can give a good review to my students if I give a good review to yours \". Again, this is why all review is, first and foremost, public in nature.","title":"3\ufe0f\u20e3 Encouraging reviews"},{"location":"essays/academia-oss/#4-qualitative-evaluations","text":"Now let's move on to specific review formats. Too often I see very long lists of checkboxes and 1-5 ratings, etc. I believe there is value in having a structured evaluation template, to make sure we more or less agree on what are the core issues we should care about. But going to the extreme of having 10 different ratings for a paper is insane! What is the difference between 6 and 7? When putting reviewers under the pressure of giving numerical scores, we are asking them to unconsciously introduce all the biases they have about that particular problem or field or approach or author. There is simply no objective way to numerically compare two different papers. A good research paper needs to have a solid methodology (correctly apply the principles of science as it is common practice in that field), provide relevant results and conclusions (either positive or negative), be feasible to reproduce by independent researchers, and have a clear presentation. Either the paper is good enough to be considered publishable, if all these aspects are covered, or it isn't. That's it. \ud83d\udc49 I prefer a simple evaluation form that asks: \" Is this aspect of the paper up to the scientific standard? , and a piece of free-form text for you to explain what is lacking in each aspect. Methodology [x] \ud83d\udc4d [ ] \ud83d\udc4e Results [x] \ud83d\udc4d [ ] \ud83d\udc4e Reproducibility [ ] \ud83d\udc4d [x] \ud83d\udc4e Presentation [ ] \ud83d\udc4d [x] \ud83d\udc4e A specific conference or journal might want to evaluate the potential impact or significance of a paper before accepting it for publishing. But impact or significance is not what Science is about. There are however legitimate cases where impact or significance is important. If you have to allocate a restricted pool of resources (e.g., grant money) of course you want to evaluate impact. Yet, I argue this is not part of the peer-review process, but a posterior analysis that each institution or publisher should do based on their specific criteria. Peer-review should be a process by which the scientific community as a whole evaluates that some research is sound science, irrespective of idiosyncrasies.","title":"4\ufe0f\u20e3 Qualitative evaluations"},{"location":"essays/academia-oss/#5-self-publishing","text":"Now that the peer-review process is completely detached from the \"mainstream\" publishing industry, who decides when is a paper ready to be published? Well, of course, the authors! It is up to the authors to determine that, given all the feedback received, they consider their work is production-ready. \ud83d\udc49 All papers would be published first in draft-mode, perhaps even before being completely written. During the draft phase, you collect all the feedback you can from peer reviews and work on the issues you consider more relevant. When you feel it is good enough (possibly because most of the recent reviews are favourable) you hit that Publish button and create a release. If some errors appear, later on, you fix them and publish another release. \ud83d\udc49 What becomes citable then? Easy, each release of each paper gets a unique DOI that will forever point to that exact version, together with all its metadata and reviews. If I cite something yours and criticize it, and you later fix it, that's OK. My critic is still valid because it points to a previous version that is indexable, and the fact that you fixed it only speaks higher of you! But wait, won't authors publish a lot of low-effort papers to engross their CVs? Well, maybe someone will but, who cares? We started by saying that, intuitively, citations should be a good measure of scientific quality. This is still true in this format. If I release a bunch of crappy papers, no one will cite them. And also, who thinks CVs are useful? Anyone trying to evaluate me as a researcher would not look into some list of titles and numbers I pasted in a Word document. They would go to my researcher profile and see my most significant work, the reviews it has received, and how my whole research process works! This doesn't mean that I get to decide my work is relevant, however. This just means I get to decide my work is ready to be consumed by the scientific community. The community will still judge my work's relevance by citing it, criticizing it, and in two more ways I left for the end.","title":"5\ufe0f\u20e3 Self-publishing"},{"location":"essays/academia-oss/#6-conferences-for-networking","text":"Now that all papers are being published by their authors, what's the purpose of scientific conferences? We can now recover their original purpose. Conferences were created as a medium to get like-minded scientists together to share their experiences and to discuss the most relevant problems of their field. But as conferences became more and more a mainstream path for getting published, their organization has become more and more about managing the peer-review process. Now that peer-review is detached from conferences, their organizers are free to focus on scooping what are the most interesting topics and the most significant results in those topics and invite those speakers they believe will the bring the biggest value. Which papers get to be presented? I think we could deal with that in two ways: \ud83d\udc49 As an organizer of a thematic conference, I would spend half of the year looking around for interesting papers to invite their authors. They would still pay for their accommodations (or their institutions would) and they would come to enjoy what's best in every conference, the networking. \ud83d\udc49 I could also open a call for papers, as usual, but authors would submit papers that are already reviewed and released. My role would be to decide, based primarily on thematic fit, what I think is more relevant for my community. \ud83d\udc49 There is even no need to attach participation to a published paper. Authors could simply submit \"talks\", possibly backed by one or more papers that support their submission, as it is already common in some conferences. This would completely reshape what conferences are for (at least in my field). There is no reason why we should wait to the top conferences of the year to be able to read papers. We would go to conferences for the chance to talk with the researchers we admire about their work. And someone asks, but how would conferences compete if they have no publication rights? Well, I argue this would be very good. Conference organizers would have to compete on the grounds of providing a better environment for networking, interesting events, nice amenities, but no one would have a monopoly on the knowledge itself. There is even no reason why the same talk cannot be presented in more than one conference if enough people are willing to listen.","title":"6\ufe0f\u20e3 Conferences for networking"},{"location":"essays/academia-oss/#7-journals-for-socializing","text":"And finally we come round to the original culprit, the infamous research journal. Now that papers are published openly, what are journals good for? Well, what they were originally designed for, socializing research! Journals were created as a means for academic societies to collect the most relevant research in a given community and publish it for a larger audience. Then commercial publishers arrived and turned science into a business, and journals became paywalled gatekeepers of knowledge, that require original research often paid with public money that they resell again for public money. The largest academic publishers often state they have costs to cover, but there is plenty of evidence that they make a significant profit. And that's OK, but if I as a journal editor want to make a profit, I'm gonna have to innovate. \ud83d\udc49 Like conferences, I could scoop around and feature the most interesting papers in some thematic issues, maybe ask the authors to give some new comments on them, prepare explainer videos, add links, and put some effort into turning those \"raw\" papers into beautifully typeset pages. \ud83d\udc49 SOTA reviews would be a nice fit for journals as well. These are not original research papers, but they often provide a lot of value by analyzing a bunch of papers and giving advice on common trends or highlighting interesting lines for future research. \ud83d\udc49 I would also have editorial articles specifically written for an issue that could summarize in layman terms about a particular subject, to introduce it to a larger audience. I would even pay scientists that are good communicators for this work. Journals would have to compete on the grounds of being good at selecting topics and papers to socialize, and provide some additional editorial value. In any case, original research papers would be owned only by their authors and would be published always with some public license (e.g., Creative Commons). This would ensure that Science belongs to the ones who ultimately pay for it, that is, society.","title":"7\ufe0f\u20e3 Journals for socializing"},{"location":"essays/academia-oss/#the-system","text":"Putting all these ideas together in a functioning system will require a lot of work. From the infrastructure point of view, I envision something aking Github, a repository of open access papers with builtin comments, reviewing, and social features. Ideally, it would also have a web UI for editing, similar to Overleaf and, of course, fully integrated with Git. I understand this might not be the best solution for academic communities that are not very closely related to software, i.e., social sciences, mostly because it could pose a significant learning curve for their members and become more of a hinder than a help. From the social point of view, kick-starting such a system would require a massive community effort. And not because of the infrastructure cost, that's the minor issue. I think the largest obstacle for this kind of paradigm shift is that a large part of the community would have to move away simultaneously from journals and conferences as the main publication channels. Otherwise, the few that start the effort will be completely disconnected from the rest of the community. I can see this happening as an effort from, say, the AI community, or any other technically-savvy collective. Tomorrow morning, all the senior scientists that publish in ICLR, ICML, NeurIPs, and ACL, could suddenly decide they want to go fully open. It would require the conference organizers to support the initiative as well. Instead of opening a call for papers, the conferences could decide that they would open a call for submissions, which should peer-reviewed and published in this new format. Some non-profit organization could be formed from within the community to provide the infrastructure. Since there will be some operative costs, this platform would require some payment, but it would be very small compared to publishing fees in most major open access journals. Also, some of the big players in the industry could support this initiative by providing hosting and infrastructure for free. This would be a big PR boost to these companies.","title":"\ud83d\udcbb The system"},{"location":"essays/academia-oss/#the-metrics","text":"We started this discussion by saying that an intuitively good metric to estimate scientific impact, i.e., citations, had become an objective and thus lost their entire meaning. But citations are not an inherently bad metric, it's just when we use citations as the one quantitative metric to compare individual researchers that we miss the entire point. Likewise, the fact that we self-publish all our papers doesn't mean that being featured in a major conference or journal is worthless. On the contrary, when everything we publish is open, being featured in a mainstream publication becomes an even better measure of impact, because it is no longer tied to my financial capacity or any other unfair advantage I might enjoy in the community. This would be very good for Third World researchers, who produce valuable science, but are often cutoff from mainstream publication for reasons completely unrelated to the quality of the research. If we can restructure the incentives and processes of Science such that they are aligned with the purpose of Science as primarily a means to improve human life, everything else would fall into place. Once researchers are free from predatory publishing practices, meaningless numerical statistics and unhealthy competition, I believe we will all focus what we love most, doing sound research for the good of mankind. Then, all those metrics that are used today will regain their meaning. Being invited to a top-tier conference would mean that what your community wants to hear from you. Being featured in a top-tier journal would mean that some editors consider your work is high-quality. And being cited often would mean that your research is producing real impact, that you are becoming a giant on whose shoulders others can stand. \ud83d\udde8\ufe0f This is a topic of which I'm very passionate about, and I want to hear your feedback. If you want to discuss this topic with me, give me a shout in Twitter . \ud83d\udeab If you find some typo, error or you have any suggestion to help me improve this page, you can suggest and edit or open an issue in Github .","title":"\ud83d\udcca The metrics"},{"location":"essays/ideas/","text":"Decisions \u00b6 most types of decisions that we notice are which-decisions which-decisions are the ones we have to make explicitely, when given two or more options, and it is impossible to not choose which recipe to cook, which brand of phone to buy, which career to study by their explicit nature, these decisions force us to think and weigh them consciously before deciding we can make the wrong choice, sure, but in hindsight, more often than not we tell ourselves that we chose the best we could with the information we had when seeing from this point of view, is hard to regret making \"the wrong choice\" if all evidence pointed in its favour so we tend to rationalize, after the fact, that whatever happened was for the better however, all the time we are making whether-decisions, the ones that come with an implicit \"do-nothing\" default option whether to start a diet, whether to call that person, whether to find a new job the default option is almost always to let things be as they are, and the active option is to change something these are dangerous ones, because if we you don't realize there is a decision involved, the default option is automatically selected everytime you are not aware of these decisions, you're making an implicit choice, the choice of inaction, of letting it be change isn't always the right choice, big endeavours often require sticking to a course of action however, most regrets don't come from choosing a wrong course of action, but from failing to act in the first place if you are choosing inaction, make sure it's a conscious choice, not a default that life is selecting for you","title":"Decisions"},{"location":"essays/ideas/#decisions","text":"most types of decisions that we notice are which-decisions which-decisions are the ones we have to make explicitely, when given two or more options, and it is impossible to not choose which recipe to cook, which brand of phone to buy, which career to study by their explicit nature, these decisions force us to think and weigh them consciously before deciding we can make the wrong choice, sure, but in hindsight, more often than not we tell ourselves that we chose the best we could with the information we had when seeing from this point of view, is hard to regret making \"the wrong choice\" if all evidence pointed in its favour so we tend to rationalize, after the fact, that whatever happened was for the better however, all the time we are making whether-decisions, the ones that come with an implicit \"do-nothing\" default option whether to start a diet, whether to call that person, whether to find a new job the default option is almost always to let things be as they are, and the active option is to change something these are dangerous ones, because if we you don't realize there is a decision involved, the default option is automatically selected everytime you are not aware of these decisions, you're making an implicit choice, the choice of inaction, of letting it be change isn't always the right choice, big endeavours often require sticking to a course of action however, most regrets don't come from choosing a wrong course of action, but from failing to act in the first place if you are choosing inaction, make sure it's a conscious choice, not a default that life is selecting for you","title":"Decisions"},{"location":"essays/opinions/","text":"How opinions should work \u00b6 Can an opinion be wrong? What would it mean to say \"yes\", or \"no\", for that matter? And what is an opinion, that makes it different from a fact? I'm gonna try to answer these questions from a very opinionated point-of-view (see what I did there?) by crafting some definitions that I think are, at least, somewhat helpful to drive this discussion. But keep in mind, this is only my opinion, in a very strict sense of opinion, that I'm gonna explain to you. To define what is an opinion , I think we have to contrast it with the complementary concept of fact . I'm gonna say there are two types of statements you can make about any subject or object: opinions and facts , and these are disjoint sets. What is a fact? \u00b6 I'm gonna define a fact as an is-statement , for example, the snow is white , or coffee is bad for health . Alternatively, I'm gonna say facts are objective statements. Facts can be right or wrong, in a very strict sense: whether they correspond to objective reality or not. However, just trying to define what objective reality is, is tremendously complex. So I will assume that most of us share a common idea of what objective reality is, even if we don't agree that some statements are part of, or can even refer to, that reality. More generally, facts can have a degree of accuracy, so they are neither right nor wrong, but somewhere in between. For example, when you say Earth is round , that is not exactly right, but is a far better description of Earth than, say, Earth is flat . Hence, among two contradictory facts (two facts that cannot be true at the same time), I'll generally say that the most accurate is \"right\", and the other, \"wrong\", but this is nuanced and contextual. The reason we care about facts, is that they are useful. Facts allow us to communicate with each other about reality. Most of us, I think, we'll agree on a large common set of facts, such that something seems to be pulling you to the ground, and that you cannot breath under water. However, many of us will not agree on some facts, even if they are a very accurate description of objective reality. This disagreement can be due to several reasons. The most obvious one, is that we have different tools to observe objective reality, such that we can be seeing different portions of the same phenomenon and thus disagreeing by a simple mismatch of data. A more complex reason is that, even when starting with the same data, we can have different reasoning processes, some better than others, and thus we can reach different conclusions. Finally, a less obvious reason, is that we don't agree on what a fact is, and one of us is taking a fact as an opinion, or an opinion as a fact. The good thing about facts is that there is, in principle, an objective way to solve any disagreement. We just have to show each other our data and describe our thought processes. This doesn't mean we can always know if a fact is right or wrong, sometimes we won't even have the technology to be able to collect or analyze that data. But at least, we should be able to agree if a fact seems right from the collective data we have gathered, or if we cannot decide yet. This is, in part, how Science works. With all this in mind, we can reach an alternative definition for facts. I would say: \ud83d\udcdd Facts are the type of statements about objective reality that can be, at least in theory, agreed upon by anyone who has access to the same set of observations and uses a principled reasoning procedure that doesn't produce contradictions. What is an opinion? \u00b6 If facts are is-statements , then I'll say opinions are ought-statements . For example, education ought to be free , or people ought to be able to say whatever they like . Alternatively, I'll say opinions are subjective statements. Do we need opinions in the first place? Can't we just get along with facts? Well, sadly, there are fundamental questions we can only answer if we allow subjective statements. The most obvious case is possibly organizing a community. For example, if you want to build a society that allows people to live together in harmony, you'll probably want a rule like people ought to respect each other's freedom . Now there's a funny thing that happens with statements. It's called the \"Hume's guillotine\", and it basically says you cannot produce an ought-statement from a chain of is-statements by pure logical reasoning. For example, you might want to say weather is cold today , and cold is bad for health , so you ought to wear a jacket . Seems reasonable, right? But the thing is, you're making an assumption, which is that I want to protect my health . And you cannot derive that as a fact from objective reality. At most, you could say you ought to protect your health , and so, you need to introduce an opinion that we need to agree on. So, bottom line is, opinions, as I have defined them here, cannot be proven logically from facts. There is no science that can validate if an opinion is \"right\", starting only from pure objective truths. You have to start with an opinion somewhere, that is taken at face value and accepted as a universal truth. A subjective truth. Keep in mind, though, that some things which may look as opinions are actually facts, and vice-versa. For example, if you say, I like coffee , that is not an opinion, that is a fact about yourself. It is a fact because we can precisely define what like means, and we can agree within reasonable accuracy, that the observations we all perceive of your coffee consumption behavior are consistent with what we refer to as liking . And if you say, democracy is a successful sociopolitical system , that is also not an opinion, it is a fact, provided you can describe what \"successful\" means in this context. It can either be a true or a false fact, depending on which metrics you pick, or somewhere in between. However, whatever the case, that fact doesn't directly imply that people ought to prefer living in a democracy , by Hume's guillotine. That is an opinion, and thus cannot be scientifically proven from objective reality. Hence, to conclude this section, I think we can come up with an alternative definition for opinion. I would say: \ud83d\udcdd Opinions are the type of statements about subjective reality, for which there are equally-valid alternative statements, that cannot be differentiated by using just true facts and any principled reasoning procedure. \u26a0\ufe0f Before we move on, I want to stress that the distinction between facts and opinions is not a linguistic issue. Is not the case that something is a fact because it is expressed as an is-statement . Or vice-versa, that I can turn a fact into an opinion by changing the way it is expressed. These syntactical \"rules\" are just a tool for us to communicate the semantic meaning of the word \"fact\" and \"opinion\", but they are not the definition. The definitions, as hard as they are to get right, rest on the key distinction that facts are statements that can be proven right or wrong, independent of your beliefs; and opinions are exactly the opposite, statements that cannot be evaluated outside of a belief system. Agree to disagree? \u00b6 Can an opinion be wrong? We could say an opinion is \"trivially wrong\" when it contradicts objective reality, or simply put, if it contradicts some true fact(s). For example, if you say electrons ought to be positively charged ; but I'll argue that those are, at the very least, useless opinions. More importantly, then, can an opinion be wrong when it doesn't contradict objective reality? For example, if you say people ought to be able to decide if they want to vaccinate their children ? Yes, vaccines are proven to help fight diseases, that is a fact. But we have to agree, as before, that people ought to do what's best for public health , and that, again, is an opinion (not necessarily held by a majority). If we say useful opinions cannot be measured against objective reality, then we can only compare opinions with other opinions. Can we agree on a set of basic opinions that determine which other opinions are valid or not? I'm gonna propose a few. I'll start by proposing that opinions ought to be only about subjective reality . This means that, in my view, you cannot have an opinion on something that can be proven scientifically. The Earth is either round or flat, and you're not entitled to believe it is any different. You can disagree with the science used to determine it. You can be a skeptic and attempt to do the experiments yourself. But you cannot believe that people are entitled to decide what charge of the electron, or what shape of planet, they want to believe in. It is not easy, at all, to determine what can be scientifically proven, so there will be statements which, at some points in time, some people will rightfully argue there is still room for opinions. But I think that in most of the statements that matter for practical purposes, even if we don't know their truth value, we can pretty much agree if they are about objective or subjective reality. Next, I'll propose that opinions ought to acknowledge their status . Hence, you cannot have an opinion, and then believe that opinion to be anything else than an opinion, i.e., you are not allowed to believe your opinion is a fact. But it's OK as long as you understand all you have is an opinion, and thus, it cannot be compared to a fact. Finally, I'm gonna argue that people ought to be allowed to have opinions that do not conform to the dominant moral dogma . I'm totally OK with, and I will fight for, people's right to hold an unpopular opinion, even if I don't agree with that specific opinion. All in all, if an opinion contradicts one of the previous three statements, I'm gonna call it a \"wrong opinion\". For example, from my point of view, you cannot say it's my opinion that vaccines don't work , but you can say people should be able to decide if they want to vaccinate . Maybe I won't agree with you, but I'll defend your right to have an opinion I disagree with, and I'll do my best to show my point of view. And this is my opinion on how opinions ought to work. \ud83d\udde8\ufe0f This is a topic of which I'm very passionate about, and I want to hear your feedback. If you want to discuss this topic with me, give me a shout in Twitter . \ud83d\udeab If you find some typo, error or you have any suggestion to help me improve this page, you can suggest and edit or open an issue in Github .","title":"Opinions"},{"location":"essays/opinions/#how-opinions-should-work","text":"Can an opinion be wrong? What would it mean to say \"yes\", or \"no\", for that matter? And what is an opinion, that makes it different from a fact? I'm gonna try to answer these questions from a very opinionated point-of-view (see what I did there?) by crafting some definitions that I think are, at least, somewhat helpful to drive this discussion. But keep in mind, this is only my opinion, in a very strict sense of opinion, that I'm gonna explain to you. To define what is an opinion , I think we have to contrast it with the complementary concept of fact . I'm gonna say there are two types of statements you can make about any subject or object: opinions and facts , and these are disjoint sets.","title":"How opinions should work"},{"location":"essays/opinions/#what-is-a-fact","text":"I'm gonna define a fact as an is-statement , for example, the snow is white , or coffee is bad for health . Alternatively, I'm gonna say facts are objective statements. Facts can be right or wrong, in a very strict sense: whether they correspond to objective reality or not. However, just trying to define what objective reality is, is tremendously complex. So I will assume that most of us share a common idea of what objective reality is, even if we don't agree that some statements are part of, or can even refer to, that reality. More generally, facts can have a degree of accuracy, so they are neither right nor wrong, but somewhere in between. For example, when you say Earth is round , that is not exactly right, but is a far better description of Earth than, say, Earth is flat . Hence, among two contradictory facts (two facts that cannot be true at the same time), I'll generally say that the most accurate is \"right\", and the other, \"wrong\", but this is nuanced and contextual. The reason we care about facts, is that they are useful. Facts allow us to communicate with each other about reality. Most of us, I think, we'll agree on a large common set of facts, such that something seems to be pulling you to the ground, and that you cannot breath under water. However, many of us will not agree on some facts, even if they are a very accurate description of objective reality. This disagreement can be due to several reasons. The most obvious one, is that we have different tools to observe objective reality, such that we can be seeing different portions of the same phenomenon and thus disagreeing by a simple mismatch of data. A more complex reason is that, even when starting with the same data, we can have different reasoning processes, some better than others, and thus we can reach different conclusions. Finally, a less obvious reason, is that we don't agree on what a fact is, and one of us is taking a fact as an opinion, or an opinion as a fact. The good thing about facts is that there is, in principle, an objective way to solve any disagreement. We just have to show each other our data and describe our thought processes. This doesn't mean we can always know if a fact is right or wrong, sometimes we won't even have the technology to be able to collect or analyze that data. But at least, we should be able to agree if a fact seems right from the collective data we have gathered, or if we cannot decide yet. This is, in part, how Science works. With all this in mind, we can reach an alternative definition for facts. I would say: \ud83d\udcdd Facts are the type of statements about objective reality that can be, at least in theory, agreed upon by anyone who has access to the same set of observations and uses a principled reasoning procedure that doesn't produce contradictions.","title":"What is a fact?"},{"location":"essays/opinions/#what-is-an-opinion","text":"If facts are is-statements , then I'll say opinions are ought-statements . For example, education ought to be free , or people ought to be able to say whatever they like . Alternatively, I'll say opinions are subjective statements. Do we need opinions in the first place? Can't we just get along with facts? Well, sadly, there are fundamental questions we can only answer if we allow subjective statements. The most obvious case is possibly organizing a community. For example, if you want to build a society that allows people to live together in harmony, you'll probably want a rule like people ought to respect each other's freedom . Now there's a funny thing that happens with statements. It's called the \"Hume's guillotine\", and it basically says you cannot produce an ought-statement from a chain of is-statements by pure logical reasoning. For example, you might want to say weather is cold today , and cold is bad for health , so you ought to wear a jacket . Seems reasonable, right? But the thing is, you're making an assumption, which is that I want to protect my health . And you cannot derive that as a fact from objective reality. At most, you could say you ought to protect your health , and so, you need to introduce an opinion that we need to agree on. So, bottom line is, opinions, as I have defined them here, cannot be proven logically from facts. There is no science that can validate if an opinion is \"right\", starting only from pure objective truths. You have to start with an opinion somewhere, that is taken at face value and accepted as a universal truth. A subjective truth. Keep in mind, though, that some things which may look as opinions are actually facts, and vice-versa. For example, if you say, I like coffee , that is not an opinion, that is a fact about yourself. It is a fact because we can precisely define what like means, and we can agree within reasonable accuracy, that the observations we all perceive of your coffee consumption behavior are consistent with what we refer to as liking . And if you say, democracy is a successful sociopolitical system , that is also not an opinion, it is a fact, provided you can describe what \"successful\" means in this context. It can either be a true or a false fact, depending on which metrics you pick, or somewhere in between. However, whatever the case, that fact doesn't directly imply that people ought to prefer living in a democracy , by Hume's guillotine. That is an opinion, and thus cannot be scientifically proven from objective reality. Hence, to conclude this section, I think we can come up with an alternative definition for opinion. I would say: \ud83d\udcdd Opinions are the type of statements about subjective reality, for which there are equally-valid alternative statements, that cannot be differentiated by using just true facts and any principled reasoning procedure. \u26a0\ufe0f Before we move on, I want to stress that the distinction between facts and opinions is not a linguistic issue. Is not the case that something is a fact because it is expressed as an is-statement . Or vice-versa, that I can turn a fact into an opinion by changing the way it is expressed. These syntactical \"rules\" are just a tool for us to communicate the semantic meaning of the word \"fact\" and \"opinion\", but they are not the definition. The definitions, as hard as they are to get right, rest on the key distinction that facts are statements that can be proven right or wrong, independent of your beliefs; and opinions are exactly the opposite, statements that cannot be evaluated outside of a belief system.","title":"What is an opinion?"},{"location":"essays/opinions/#agree-to-disagree","text":"Can an opinion be wrong? We could say an opinion is \"trivially wrong\" when it contradicts objective reality, or simply put, if it contradicts some true fact(s). For example, if you say electrons ought to be positively charged ; but I'll argue that those are, at the very least, useless opinions. More importantly, then, can an opinion be wrong when it doesn't contradict objective reality? For example, if you say people ought to be able to decide if they want to vaccinate their children ? Yes, vaccines are proven to help fight diseases, that is a fact. But we have to agree, as before, that people ought to do what's best for public health , and that, again, is an opinion (not necessarily held by a majority). If we say useful opinions cannot be measured against objective reality, then we can only compare opinions with other opinions. Can we agree on a set of basic opinions that determine which other opinions are valid or not? I'm gonna propose a few. I'll start by proposing that opinions ought to be only about subjective reality . This means that, in my view, you cannot have an opinion on something that can be proven scientifically. The Earth is either round or flat, and you're not entitled to believe it is any different. You can disagree with the science used to determine it. You can be a skeptic and attempt to do the experiments yourself. But you cannot believe that people are entitled to decide what charge of the electron, or what shape of planet, they want to believe in. It is not easy, at all, to determine what can be scientifically proven, so there will be statements which, at some points in time, some people will rightfully argue there is still room for opinions. But I think that in most of the statements that matter for practical purposes, even if we don't know their truth value, we can pretty much agree if they are about objective or subjective reality. Next, I'll propose that opinions ought to acknowledge their status . Hence, you cannot have an opinion, and then believe that opinion to be anything else than an opinion, i.e., you are not allowed to believe your opinion is a fact. But it's OK as long as you understand all you have is an opinion, and thus, it cannot be compared to a fact. Finally, I'm gonna argue that people ought to be allowed to have opinions that do not conform to the dominant moral dogma . I'm totally OK with, and I will fight for, people's right to hold an unpopular opinion, even if I don't agree with that specific opinion. All in all, if an opinion contradicts one of the previous three statements, I'm gonna call it a \"wrong opinion\". For example, from my point of view, you cannot say it's my opinion that vaccines don't work , but you can say people should be able to decide if they want to vaccinate . Maybe I won't agree with you, but I'll defend your right to have an opinion I disagree with, and I'll do my best to show my point of view. And this is my opinion on how opinions ought to work. \ud83d\udde8\ufe0f This is a topic of which I'm very passionate about, and I want to hear your feedback. If you want to discuss this topic with me, give me a shout in Twitter . \ud83d\udeab If you find some typo, error or you have any suggestion to help me improve this page, you can suggest and edit or open an issue in Github .","title":"Agree to disagree?"},{"location":"essays/teaching/","text":"Why I am a teacher \u00b6 When people ask me who I am, the short answer is \"a teacher\", even though I do a bunch of other stuff. This is why. I didn't know I wanted to be a college teacher until about 3rd year of my major. Even then, the thought came slowly, something like \"yeah teaching could nice, but I may enjoy something else as well\". By the end of my 5th year I was certain there was nothing else that could fill me up. I've been in front of a classroom ever since, at least twice a week one semester a year. And I've loved every minute of it. Students are the same everywhere. They have hopes, dreams, and a lot of misconceptions. They come thinking they want to be something (an engineer, a computer scientist, a journalist, a lawyer, ...), because they think that choice will lead them to do something (solve problems, work at a large company, travel the world, ...). So they focus on that search: what is the something they want to do, hence, the something they want to be? My main task is to try and convince them otherwise. The search is not about a something . It's about a someone . You need to come to college to discover who you want to be instead of what you want to do. Engineer, computer scientist, lawyer, all of those are just labels that somehow try to average over the set of things that people who label themselves that way like to do. Don't get me wrong, labels are important. They help us organize and understand the world. But if there is one occasion when you don't want a simple label to smooth away all the tiny details, is when choosing who you want to be for life. So when people ask me \"are you an engineer, a scientist, or a philosopher?\" I answer yes. I'm mostly a scientist, because I do more research than the average engineer; but I'm also an engineer, because I solve more problems than the average philosopher; and, I'm also a philosopher, because I like to think more about the implications of my decisions than the average scientist. I'm also a lot of other things, if you ask. It's not that I'm somehow \"better\" than any of these individual labels, it's simply that I choose to be my own brew of these \"things\", taking from each what I like and dumping what I don't. To a simple question ( what are you?) I can only give a simple answer (yes). If you want the details, you'll have to ask the deeper question: who are you? My first day in class every year, I like to throw a simple question at my students: who do you want to be? Most of them answer with a combination of whats. I want to be this or I want to do that. Over the year, some start to discover they want to be someone , not just something. They start dumping the labels and start answering to this question not with things (I want to be a programmer) but with choices (I want to solve this problem, I want to cure this disease, I want to build this thing). Those choices become the who they strive for. Eventually, even if unconsciously, most reach this state. A tiny fraction of them will consciously acknowledge it. And an even smaller fraction, maybe one or two a year, sometimes none, will come one day and say to me something like \"thank you for helping me find my who\", even if not with those exact words. And that's it. That small moment when I realize someone found a better version of themselves, and I had a tiny bit of influence. That's my reward. That's who I am.","title":"Why I love teaching"},{"location":"essays/teaching/#why-i-am-a-teacher","text":"When people ask me who I am, the short answer is \"a teacher\", even though I do a bunch of other stuff. This is why. I didn't know I wanted to be a college teacher until about 3rd year of my major. Even then, the thought came slowly, something like \"yeah teaching could nice, but I may enjoy something else as well\". By the end of my 5th year I was certain there was nothing else that could fill me up. I've been in front of a classroom ever since, at least twice a week one semester a year. And I've loved every minute of it. Students are the same everywhere. They have hopes, dreams, and a lot of misconceptions. They come thinking they want to be something (an engineer, a computer scientist, a journalist, a lawyer, ...), because they think that choice will lead them to do something (solve problems, work at a large company, travel the world, ...). So they focus on that search: what is the something they want to do, hence, the something they want to be? My main task is to try and convince them otherwise. The search is not about a something . It's about a someone . You need to come to college to discover who you want to be instead of what you want to do. Engineer, computer scientist, lawyer, all of those are just labels that somehow try to average over the set of things that people who label themselves that way like to do. Don't get me wrong, labels are important. They help us organize and understand the world. But if there is one occasion when you don't want a simple label to smooth away all the tiny details, is when choosing who you want to be for life. So when people ask me \"are you an engineer, a scientist, or a philosopher?\" I answer yes. I'm mostly a scientist, because I do more research than the average engineer; but I'm also an engineer, because I solve more problems than the average philosopher; and, I'm also a philosopher, because I like to think more about the implications of my decisions than the average scientist. I'm also a lot of other things, if you ask. It's not that I'm somehow \"better\" than any of these individual labels, it's simply that I choose to be my own brew of these \"things\", taking from each what I like and dumping what I don't. To a simple question ( what are you?) I can only give a simple answer (yes). If you want the details, you'll have to ask the deeper question: who are you? My first day in class every year, I like to throw a simple question at my students: who do you want to be? Most of them answer with a combination of whats. I want to be this or I want to do that. Over the year, some start to discover they want to be someone , not just something. They start dumping the labels and start answering to this question not with things (I want to be a programmer) but with choices (I want to solve this problem, I want to cure this disease, I want to build this thing). Those choices become the who they strive for. Eventually, even if unconsciously, most reach this state. A tiny fraction of them will consciously acknowledge it. And an even smaller fraction, maybe one or two a year, sometimes none, will come one day and say to me something like \"thank you for helping me find my who\", even if not with those exact words. And that's it. That small moment when I realize someone found a better version of themselves, and I had a tiny bit of influence. That's my reward. That's who I am.","title":"Why I am a teacher"},{"location":"essays/team-playing/","text":"It's all about team-playing \u00b6 Modern higher education is all about competences and skills. In the process, we are losing some very bright people who just don't fit this narrow-minded model of professionalism. Modern education started with the Industrial Revolution and the need to graduate tons of skilled workers to carry on the same tasks over and over. Previously, education was only for the brightest and/or luckiest, and usually consisted of a very custom path through which a tutor would guide you. Nowadays in universities all around the world, we have reduced students to numbers, grades, percentages, as if we were producing computer chips or combustion engines. Efficiency is all that matters. To achieve the highest possible efficiency, all around the world we educators have become engineers of sorts. We designed what we call a \"model of the professional\", which is a set of skills and competencies that an abstract ideal professional should have. Then we designed an evaluation metric he micro-average of a ton of super-narrow scores that measure super-specific skills such as taking a derivative or coding a recursive function. Finally, we designed a pipeline that takes students on one end and produces \"professionals\" on the other end. Those \"smart\" enough to learn to beat the system get the highest grades and are stamped with an abstract generic title of Computer Scientist, Medical Doctor, Lawyer, very much like a certificate of quality in a generic bottle of wine. This system is deeply flawed, and educators all over the world know it and have been discussing it for a long time. It's hard to change for many reasons, the least of which is the lack of teachers willing to dump the generic instruction set and craft custom learning paths for their students. I think this system is based on two basic assumptions, intuitive but flawed. Changing those assumptions could shed light on ways to improve the system. 1\ufe0f\u20e3 The first assumption is that students are a blank slate that when fed through this generic pipeline we call higher education will be magically morphed into this generic professional we designed. This is wrong for so many reasons that is hard to acknowledge it as a basic assumption of our system. \ud83d\udc49 Ask any university professor and they will all tell you the same: All students are different. They all have different skills, interests and biases. They all require a different approach to get the most out of them. And almost all of them, when given the chance and the right environment, will become the best versions of themselves. Yet time and time again we treat them as generic droids on which we can dump a generic course and expect a generic performance in return. 2\ufe0f\u20e3 The second assumption, I think, is harder to spot, because of the way the university is disconnected from real life all around the world. We educators think that society wants this \"model of the professional\" because we think that a hospital needs 100 equally generic doctors, and a software company needs 100 equally generic programmers. However, this is also wrong at many levels. \ud83d\udc49 Everywhere we ask in the industry we keep hearing the same: we need unique people with unique skills that bring something new to the team. It's like trying to build an ensemble out of 100 equal models. You get much better results with a variety of approaches to the same problem, than with an array of 100 exactly equal programs. Yet we keep translating what society asks into skills and competencies. They tell us they need unique people, and we add \"uniqueness\" to the set of generic skills we want to teach with our generic college programs! So let's dump those two assumptions and acknowledge that we have a bunch of different kids with different interests and capabilities, and we need to turn them into a bunch of different professionals with different mindsets and skills. Now the question is how on earth can we do that? As engineers, we need to design a streamlined pipeline and a proper evaluation metric. And we need to do that, unfortunately, because there are so many more students than teachers that we cannot hope to be the Aristotle to each Alexander, and that's not about to change in the near future. I think one possible strategy is to focus on a single evaluation metric, and a single skill: \ud83d\udca1 Strive to transform every student into an effective team-player. Let's take it piece by piece. Every student is different, so everyone will have a different set of potential capabilities that could make them effective team-players. If we encourage those specific capabilities on each student, we are giving each one a different learning path. This one will focus on improving her analytical skills, that one will focus on improving his management skills, the other one her social skills, and so on. Each one is focusing on their own most interesting, most desirable version of themselves. On the other hand, everyone is optimizing the same metric, being a good team-player, whatever the team. Give them back to society and they will fit in the right spot. The one spot that needs that specific mindset. Almost all low-hanging fruits that a single bright person could take are already taken, the problems that are left to solve as a society are the hard problems, and they all require teamwork. The easier problems are being automated away as I type. So, I argue, the most important skill today is being an effective team-player. If we strive to turn our students into exactly that, we are giving them the best education possible, and we are giving society the best possible return on that investment. The final question is how exactly do we do that? How do we discover what makes every student unique and valuable in a team? Isn't that the same Aristotle & Alexander dilemma? I think a possible solution is simply to let each of them discover it by themselves. As educators, instead of trying to tell everyone what to do, let's focus on designing learning environments, comfortable for every student to explore their own skills and capabilities, and to decide the best way for them to serve the team. And let's evaluate them on co-op instead of solo so that, when trying to beat the system, they will effectively optimize what we, the rest of the world, need them to be good at. \ud83d\udde8\ufe0f This is a topic of which I'm very passionate about, and I want to hear your feedback. If you want to discuss this topic with me, give me a shout in Twitter . \ud83d\udeab If you find some typo, error or you have any suggestion to help me improve this page, you can suggest and edit or open an issue in Github .","title":"Team-playing"},{"location":"essays/team-playing/#its-all-about-team-playing","text":"Modern higher education is all about competences and skills. In the process, we are losing some very bright people who just don't fit this narrow-minded model of professionalism. Modern education started with the Industrial Revolution and the need to graduate tons of skilled workers to carry on the same tasks over and over. Previously, education was only for the brightest and/or luckiest, and usually consisted of a very custom path through which a tutor would guide you. Nowadays in universities all around the world, we have reduced students to numbers, grades, percentages, as if we were producing computer chips or combustion engines. Efficiency is all that matters. To achieve the highest possible efficiency, all around the world we educators have become engineers of sorts. We designed what we call a \"model of the professional\", which is a set of skills and competencies that an abstract ideal professional should have. Then we designed an evaluation metric he micro-average of a ton of super-narrow scores that measure super-specific skills such as taking a derivative or coding a recursive function. Finally, we designed a pipeline that takes students on one end and produces \"professionals\" on the other end. Those \"smart\" enough to learn to beat the system get the highest grades and are stamped with an abstract generic title of Computer Scientist, Medical Doctor, Lawyer, very much like a certificate of quality in a generic bottle of wine. This system is deeply flawed, and educators all over the world know it and have been discussing it for a long time. It's hard to change for many reasons, the least of which is the lack of teachers willing to dump the generic instruction set and craft custom learning paths for their students. I think this system is based on two basic assumptions, intuitive but flawed. Changing those assumptions could shed light on ways to improve the system. 1\ufe0f\u20e3 The first assumption is that students are a blank slate that when fed through this generic pipeline we call higher education will be magically morphed into this generic professional we designed. This is wrong for so many reasons that is hard to acknowledge it as a basic assumption of our system. \ud83d\udc49 Ask any university professor and they will all tell you the same: All students are different. They all have different skills, interests and biases. They all require a different approach to get the most out of them. And almost all of them, when given the chance and the right environment, will become the best versions of themselves. Yet time and time again we treat them as generic droids on which we can dump a generic course and expect a generic performance in return. 2\ufe0f\u20e3 The second assumption, I think, is harder to spot, because of the way the university is disconnected from real life all around the world. We educators think that society wants this \"model of the professional\" because we think that a hospital needs 100 equally generic doctors, and a software company needs 100 equally generic programmers. However, this is also wrong at many levels. \ud83d\udc49 Everywhere we ask in the industry we keep hearing the same: we need unique people with unique skills that bring something new to the team. It's like trying to build an ensemble out of 100 equal models. You get much better results with a variety of approaches to the same problem, than with an array of 100 exactly equal programs. Yet we keep translating what society asks into skills and competencies. They tell us they need unique people, and we add \"uniqueness\" to the set of generic skills we want to teach with our generic college programs! So let's dump those two assumptions and acknowledge that we have a bunch of different kids with different interests and capabilities, and we need to turn them into a bunch of different professionals with different mindsets and skills. Now the question is how on earth can we do that? As engineers, we need to design a streamlined pipeline and a proper evaluation metric. And we need to do that, unfortunately, because there are so many more students than teachers that we cannot hope to be the Aristotle to each Alexander, and that's not about to change in the near future. I think one possible strategy is to focus on a single evaluation metric, and a single skill: \ud83d\udca1 Strive to transform every student into an effective team-player. Let's take it piece by piece. Every student is different, so everyone will have a different set of potential capabilities that could make them effective team-players. If we encourage those specific capabilities on each student, we are giving each one a different learning path. This one will focus on improving her analytical skills, that one will focus on improving his management skills, the other one her social skills, and so on. Each one is focusing on their own most interesting, most desirable version of themselves. On the other hand, everyone is optimizing the same metric, being a good team-player, whatever the team. Give them back to society and they will fit in the right spot. The one spot that needs that specific mindset. Almost all low-hanging fruits that a single bright person could take are already taken, the problems that are left to solve as a society are the hard problems, and they all require teamwork. The easier problems are being automated away as I type. So, I argue, the most important skill today is being an effective team-player. If we strive to turn our students into exactly that, we are giving them the best education possible, and we are giving society the best possible return on that investment. The final question is how exactly do we do that? How do we discover what makes every student unique and valuable in a team? Isn't that the same Aristotle & Alexander dilemma? I think a possible solution is simply to let each of them discover it by themselves. As educators, instead of trying to tell everyone what to do, let's focus on designing learning environments, comfortable for every student to explore their own skills and capabilities, and to decide the best way for them to serve the team. And let's evaluate them on co-op instead of solo so that, when trying to beat the system, they will effectively optimize what we, the rest of the world, need them to be good at. \ud83d\udde8\ufe0f This is a topic of which I'm very passionate about, and I want to hear your feedback. If you want to discuss this topic with me, give me a shout in Twitter . \ud83d\udeab If you find some typo, error or you have any suggestion to help me improve this page, you can suggest and edit or open an issue in Github .","title":"It's all about team-playing"},{"location":"guides/technical-writing/","text":"A Guide to Technical Writing \u00b6 technical writing is focused on making onself understood, not to demostrate one's knowledge or abilities, nor to convince the reader of something technical writing is a form of technical communication, probably the most common form, but not the only one (e.g., talks and interviews) technical writing is centered around discussing a technical topic a technical topic has a especialized audience (it is not interesting to anyone) and a especialized language a technical topic has prerequisites or background knowledge expected from the reader before the discussion begins the discussion around a technical topic is fundamentally objective, that is, can be backed with objective arguments and/or evidence the reader is expected to be open to the discussion but critically thinking not every type of writing is technical writing a political discussion about which party's ideology is \"better\" is not a technical discussion a philosophical discussion about which moral system is \"better\" is not a technical discussion even for non-technical discussions, some of the advice in this guide can be helpfull there are three main styles in technical writing: argumentative, informative, and didactic technical writing often combines the three styles in different degrees the argumentative style focuses on discussing with the reader a point of view in contrast with alternatives a staple of the argumentative style is that the author makes the point of argument explicit, there is no \"hidden\" motivation the informative style focuses on providing accurate information, with no opinion attached a staple of the informative style is to back up claims with verifiable evidence the didactic style focuses on making the reader learn something new according to the motivation of the author, different structures and styles are more effective technical writing does not hinges on genious or muses, anyone can write compelling articles following a process","title":"A Guide to Technical Writing"},{"location":"guides/technical-writing/#a-guide-to-technical-writing","text":"technical writing is focused on making onself understood, not to demostrate one's knowledge or abilities, nor to convince the reader of something technical writing is a form of technical communication, probably the most common form, but not the only one (e.g., talks and interviews) technical writing is centered around discussing a technical topic a technical topic has a especialized audience (it is not interesting to anyone) and a especialized language a technical topic has prerequisites or background knowledge expected from the reader before the discussion begins the discussion around a technical topic is fundamentally objective, that is, can be backed with objective arguments and/or evidence the reader is expected to be open to the discussion but critically thinking not every type of writing is technical writing a political discussion about which party's ideology is \"better\" is not a technical discussion a philosophical discussion about which moral system is \"better\" is not a technical discussion even for non-technical discussions, some of the advice in this guide can be helpfull there are three main styles in technical writing: argumentative, informative, and didactic technical writing often combines the three styles in different degrees the argumentative style focuses on discussing with the reader a point of view in contrast with alternatives a staple of the argumentative style is that the author makes the point of argument explicit, there is no \"hidden\" motivation the informative style focuses on providing accurate information, with no opinion attached a staple of the informative style is to back up claims with verifiable evidence the didactic style focuses on making the reader learn something new according to the motivation of the author, different structures and styles are more effective technical writing does not hinges on genious or muses, anyone can write compelling articles following a process","title":"A Guide to Technical Writing"},{"location":"guides/technical-writing/ideas/","text":"how to make a technical literature review \u00b6 a literature review can be for several reasons: informative (a survey), critical (to make a point), support (to complement an ongoing research) an informative review (survey) should highlight the key concepts and interesting connections more than just tell what has been done an critical review should focus on the key questions that remain unsolved and point at possible areas where to look for an answer a support review should highlight an existing problem and establish how a new contribution fits a larger picture how to find which papers to read start with a seed of recent papers on the topic, if you have surveys even better when in doubt, ask the experts: find the best papers recent in the conferences and journals of the area these papers are seed nodes in a directed graph, what you want to discover is the important overarching ideas follow other papers' literature reviews to find who they link to use tools like google scholar, connected papers, and semantic scholar, to quickly navigate through the references what is considered SOTA or relevant changes for every field but you should keep a healthy balance of new stuff and seminal stuff as a general rule, the older a paper, the more important it has to be for you to consider it citations are a somewhat good proxy for relevance, but not exactly, often a less cited paper is the original idea but a survey or a follow-up paper is the one that got most citations also citations numbers across different communities are in general not comparable, be critical make a database of every paper you read (can be an excel sheet) for each paper, annotate author and characteristics characteristics can grow and / or refactor as you read more read only what you need to fill-in the table bookmark the papers you will need to read further identify trends and clusters of similar ideas to find an underlying structure create a taxonomy or structure from those characteristics make an outline or mind map, adding to each idea the key citations make sure the outline focuses on the important bits, according to what type of review it is when possible add tables or graphs that summarize the important parts qualitative comparison between existing approaches highlighting when each is applicable, advantages and disadvantages graphical representation of the taxonomical relations, which ideas lead to which ideas quantitative comparison according to some interesting metrics (this can require you to actually make experiments and measure things) end the review with key highlights if its a survey, focus on the interesting open questions, separate the low-hanging fruits from the difficult ones if its a critic, focus on what you discovered, e.g., what are the key blockers or what have people missed if its a support, clearly highlight the something missing where your contributions fit, and how the existent results support your hipothesis how to read technical papers \u00b6 how to write an original research paper (in Computer Science) \u00b6","title":"how to make a technical literature review"},{"location":"guides/technical-writing/ideas/#how-to-make-a-technical-literature-review","text":"a literature review can be for several reasons: informative (a survey), critical (to make a point), support (to complement an ongoing research) an informative review (survey) should highlight the key concepts and interesting connections more than just tell what has been done an critical review should focus on the key questions that remain unsolved and point at possible areas where to look for an answer a support review should highlight an existing problem and establish how a new contribution fits a larger picture how to find which papers to read start with a seed of recent papers on the topic, if you have surveys even better when in doubt, ask the experts: find the best papers recent in the conferences and journals of the area these papers are seed nodes in a directed graph, what you want to discover is the important overarching ideas follow other papers' literature reviews to find who they link to use tools like google scholar, connected papers, and semantic scholar, to quickly navigate through the references what is considered SOTA or relevant changes for every field but you should keep a healthy balance of new stuff and seminal stuff as a general rule, the older a paper, the more important it has to be for you to consider it citations are a somewhat good proxy for relevance, but not exactly, often a less cited paper is the original idea but a survey or a follow-up paper is the one that got most citations also citations numbers across different communities are in general not comparable, be critical make a database of every paper you read (can be an excel sheet) for each paper, annotate author and characteristics characteristics can grow and / or refactor as you read more read only what you need to fill-in the table bookmark the papers you will need to read further identify trends and clusters of similar ideas to find an underlying structure create a taxonomy or structure from those characteristics make an outline or mind map, adding to each idea the key citations make sure the outline focuses on the important bits, according to what type of review it is when possible add tables or graphs that summarize the important parts qualitative comparison between existing approaches highlighting when each is applicable, advantages and disadvantages graphical representation of the taxonomical relations, which ideas lead to which ideas quantitative comparison according to some interesting metrics (this can require you to actually make experiments and measure things) end the review with key highlights if its a survey, focus on the interesting open questions, separate the low-hanging fruits from the difficult ones if its a critic, focus on what you discovered, e.g., what are the key blockers or what have people missed if its a support, clearly highlight the something missing where your contributions fit, and how the existent results support your hipothesis","title":"how to make a technical literature review"},{"location":"guides/technical-writing/ideas/#how-to-read-technical-papers","text":"","title":"how to read technical papers"},{"location":"guides/technical-writing/ideas/#how-to-write-an-original-research-paper-in-computer-science","text":"","title":"how to write an original research paper (in Computer Science)"},{"location":"guides/technical-writing/process/","text":"Technical Writing - The Process \u00b6 Some think writing requires inspiration, a muse, or special talent. While these may (or not) be necessary for writing a fiction masterpiece, they certainly aren't needed to write a decent technical piece. Technical writing is a skill that can be learned and perfected with practice, and there are methods to help you move forward. The main blockers I've struggled with during technical writing are: Lack of original ideas. Missing a clear and unified structure. Running out of ideas or motivation halfway through. Dull and boring writing, lacking impact. A key ingredient to overcoming these blockers is to iterate a lot. It is impossible for most of us to come up with original, coherent, and impactful ideas, all at once. Instead, you start with a bunch of unconnected ideas, and you relentlessly edit, rewrite, and improve. The other ingredient is to know when to stop. No technical article is ever ready, you just decide to publish it when it's good enough. There are two reasons why this relentless editing is difficult: It is very hard to edit your own words once you get too attached to them. The longer you spend thinking about a specific sentence, the harder it will be for you to let it go. Besides, language is contextual, every sentence is connected with its surroundings. The longer a sentence lives, there harder it is to refactor it without compromising the story in which it is embedded. Hence, we need to delay the moment when ideas grow roots to as late as possible. We want them to float freely and bounce around as long as possible. And we want them to grow roots and entrench themselves into a coherent narrative at just the right time. Here's a losely-defined process that works for me. It starts with brain-dumping the key ideas before threading an underlying structure . Then, we'll make three passes, one to fill with content , another to prune for clarity and a third one to polish for impact . And that's it, three drafts and we're ready to publish. Brain-dump key ideas \u00b6 The first step is to come up with as many ideas as possible, regardless of their quality. You've been thinking about a topic for a while, and you already have a bunch of unconnected thoughts you want to explore. Sit down and write them, one by one, as they come up to your mind. Forget about structure, order, story, coherence; nothing matters beyond individual ideas. Sometimes an idea requires one sentence, sometimes a couple, and sometimes barely a phrase. Let each idea float freely, unconnected with the rest. Describe it with just the few words you need to understand it when you come back later. It doesn't need to make sense to anyone but you. The key insight is to not spend too much time thinking about any particular idea. Do not refine them, do not rewrite them to make them clearer beyond their initial conception. Open your mind and let ideas pour out as unchallenged as you can possibly do. Thread a structure \u00b6 Now you have a heap of ideas, some potentially good, some definitely bad, some ugly. They are floating freely, unconnected, unperturbed by their companions. Most of these ideas won't see the light of day, but you still don't know which ones will make the cut. The next step is to thread a meaningful structure around the ones that really matter. Your task is to reorder ideas so that similar topics start to cluster and a structure begins to emerge. Since ideas are slim and free, you can move them around, group them into sections and subsections as you wish. Play around with different structures here and there; maybe save copies to revisit later. Resist explaining or refining the ideas. Keep them slim. Remember that fat ideas grow roots. They become heavy and hard to reorder into a different structure. They attach to their context. As you play with different structures, you will converge into a definite form, a story where some ideas fit neatly, and some don't. Cut these loose without a second thought. Ideas are cheap anyway. That's why we want them to be slim in the first place, and why we didn't let ourselves get attached to them. At the end of this step you'll have a blueprint, a kind of skeleton that threads across all the remaining ideas, connecting them into a larger narrative. You'll have removed any idea that didn't fit, and possibly sprinkled here and there a few supporting ones where the structure felt less solid. They are ready to grow roots. Fill with content \u00b6 Now that you have a solid structure, it's time to let ideas flourish. Expand every idea into a paragraph, or two, or three, as long as you need. You're only interested in quantity now, not quality. Try and respect the structure you already have in place. Why? Because if you deviate too much from it, you'll be back at square one, lacking a clear structure, but with bloated paragraphs instead of slim ideas. And we already know this is bad: ideas are easy to move around, but paragraphs grow roots, and they are almost impossible to refactor once they start taking shape. Spend as little time as possible thinking about any given sentence. Just write them down as they come, without concern for clarity, conciseness, or impact. Why? Because the longer you linger on a sentence, the more you'll grow attached to it, and the harder it will be to cut it down later. Keep adding content as long as you have the energy to do it. At some point, ideas just stop growing, and the struggle to extract meaning from them becomes so big that you spend more time thinking than writing. Stop there. Resist any urge to double-think about what to write. If it's not coming easily, it's time to rest. You might need a couple more rounds to cover all the ideas in your outline, so take your time to recharge. Prune for clarity \u00b6 At this point you are saying everything you want to say, but cluttered and densely. Most ideas deas are overexposed, overgrown, touching their surroundings too much. You'll have repeated the same stuff over and over. It's time to prune. Aim to cut every paragraph down to half its size. First, just remove it to see if you needed it. If you don't miss it, it's gone for good. Otherwise, put it back and do the same with every sentence. You'll find that half your content can be cut clean. Now go over what's left and rewrite to make every idea as clear-cut as possible. Remove everything nonesential. Simplify tenses. Put the action at the forefront of each sentence and paragraph. Stop when you feel there's nothing unnecesary left. Every word must have a job: if you remove it, something breaks. You can come back a couple times and remove a word here and there. You'll know when it's time to move on. Polish for impact \u00b6 You have a lean story now, nothing unnecessary, but it's probably kind of gray, dull, boring; using words like \"probably\" and \"kind of\". It's time to make it blossom. In each paragraph, think how do you want the reader to feel: informed, impressed, intrigued, excited? Look for the words that conflict with that desired tone. Identify the key ideas in your narrative that should feel like a punch in the face. Build momemtum towards them, lowering the tone of the previous paragraphs to make them smoother, and then deliver that punch with maximum impact. Rewrite those key sentences to make them more impactful, ressonant, bolder. Drop boring qualifiers: instead of \"very important\" say \"crucial\". Remove any brakes and cushions. Finally, find the most relevant idea you want to deliver. Go back and add hints here and there to build towards it: drop an open question early on, or a subtle mention in a related passage. Repeat it a few times to make it ressonate. And that's it. When you're done with this third draft, your text is ready to publish. Put it out there and move on to the next piece. I purposefully left out anything concerning getting feedback, and not because it's irrelevant. On the contrary, getting feedback early on is critical. But this is a sufficiently complex topic on its own, so I'll leave it for another ocasion.","title":"The Process"},{"location":"guides/technical-writing/process/#technical-writing-the-process","text":"Some think writing requires inspiration, a muse, or special talent. While these may (or not) be necessary for writing a fiction masterpiece, they certainly aren't needed to write a decent technical piece. Technical writing is a skill that can be learned and perfected with practice, and there are methods to help you move forward. The main blockers I've struggled with during technical writing are: Lack of original ideas. Missing a clear and unified structure. Running out of ideas or motivation halfway through. Dull and boring writing, lacking impact. A key ingredient to overcoming these blockers is to iterate a lot. It is impossible for most of us to come up with original, coherent, and impactful ideas, all at once. Instead, you start with a bunch of unconnected ideas, and you relentlessly edit, rewrite, and improve. The other ingredient is to know when to stop. No technical article is ever ready, you just decide to publish it when it's good enough. There are two reasons why this relentless editing is difficult: It is very hard to edit your own words once you get too attached to them. The longer you spend thinking about a specific sentence, the harder it will be for you to let it go. Besides, language is contextual, every sentence is connected with its surroundings. The longer a sentence lives, there harder it is to refactor it without compromising the story in which it is embedded. Hence, we need to delay the moment when ideas grow roots to as late as possible. We want them to float freely and bounce around as long as possible. And we want them to grow roots and entrench themselves into a coherent narrative at just the right time. Here's a losely-defined process that works for me. It starts with brain-dumping the key ideas before threading an underlying structure . Then, we'll make three passes, one to fill with content , another to prune for clarity and a third one to polish for impact . And that's it, three drafts and we're ready to publish.","title":"Technical Writing - The Process"},{"location":"guides/technical-writing/process/#brain-dump-key-ideas","text":"The first step is to come up with as many ideas as possible, regardless of their quality. You've been thinking about a topic for a while, and you already have a bunch of unconnected thoughts you want to explore. Sit down and write them, one by one, as they come up to your mind. Forget about structure, order, story, coherence; nothing matters beyond individual ideas. Sometimes an idea requires one sentence, sometimes a couple, and sometimes barely a phrase. Let each idea float freely, unconnected with the rest. Describe it with just the few words you need to understand it when you come back later. It doesn't need to make sense to anyone but you. The key insight is to not spend too much time thinking about any particular idea. Do not refine them, do not rewrite them to make them clearer beyond their initial conception. Open your mind and let ideas pour out as unchallenged as you can possibly do.","title":"Brain-dump key ideas"},{"location":"guides/technical-writing/process/#thread-a-structure","text":"Now you have a heap of ideas, some potentially good, some definitely bad, some ugly. They are floating freely, unconnected, unperturbed by their companions. Most of these ideas won't see the light of day, but you still don't know which ones will make the cut. The next step is to thread a meaningful structure around the ones that really matter. Your task is to reorder ideas so that similar topics start to cluster and a structure begins to emerge. Since ideas are slim and free, you can move them around, group them into sections and subsections as you wish. Play around with different structures here and there; maybe save copies to revisit later. Resist explaining or refining the ideas. Keep them slim. Remember that fat ideas grow roots. They become heavy and hard to reorder into a different structure. They attach to their context. As you play with different structures, you will converge into a definite form, a story where some ideas fit neatly, and some don't. Cut these loose without a second thought. Ideas are cheap anyway. That's why we want them to be slim in the first place, and why we didn't let ourselves get attached to them. At the end of this step you'll have a blueprint, a kind of skeleton that threads across all the remaining ideas, connecting them into a larger narrative. You'll have removed any idea that didn't fit, and possibly sprinkled here and there a few supporting ones where the structure felt less solid. They are ready to grow roots.","title":"Thread a structure"},{"location":"guides/technical-writing/process/#fill-with-content","text":"Now that you have a solid structure, it's time to let ideas flourish. Expand every idea into a paragraph, or two, or three, as long as you need. You're only interested in quantity now, not quality. Try and respect the structure you already have in place. Why? Because if you deviate too much from it, you'll be back at square one, lacking a clear structure, but with bloated paragraphs instead of slim ideas. And we already know this is bad: ideas are easy to move around, but paragraphs grow roots, and they are almost impossible to refactor once they start taking shape. Spend as little time as possible thinking about any given sentence. Just write them down as they come, without concern for clarity, conciseness, or impact. Why? Because the longer you linger on a sentence, the more you'll grow attached to it, and the harder it will be to cut it down later. Keep adding content as long as you have the energy to do it. At some point, ideas just stop growing, and the struggle to extract meaning from them becomes so big that you spend more time thinking than writing. Stop there. Resist any urge to double-think about what to write. If it's not coming easily, it's time to rest. You might need a couple more rounds to cover all the ideas in your outline, so take your time to recharge.","title":"Fill with content"},{"location":"guides/technical-writing/process/#prune-for-clarity","text":"At this point you are saying everything you want to say, but cluttered and densely. Most ideas deas are overexposed, overgrown, touching their surroundings too much. You'll have repeated the same stuff over and over. It's time to prune. Aim to cut every paragraph down to half its size. First, just remove it to see if you needed it. If you don't miss it, it's gone for good. Otherwise, put it back and do the same with every sentence. You'll find that half your content can be cut clean. Now go over what's left and rewrite to make every idea as clear-cut as possible. Remove everything nonesential. Simplify tenses. Put the action at the forefront of each sentence and paragraph. Stop when you feel there's nothing unnecesary left. Every word must have a job: if you remove it, something breaks. You can come back a couple times and remove a word here and there. You'll know when it's time to move on.","title":"Prune for clarity"},{"location":"guides/technical-writing/process/#polish-for-impact","text":"You have a lean story now, nothing unnecessary, but it's probably kind of gray, dull, boring; using words like \"probably\" and \"kind of\". It's time to make it blossom. In each paragraph, think how do you want the reader to feel: informed, impressed, intrigued, excited? Look for the words that conflict with that desired tone. Identify the key ideas in your narrative that should feel like a punch in the face. Build momemtum towards them, lowering the tone of the previous paragraphs to make them smoother, and then deliver that punch with maximum impact. Rewrite those key sentences to make them more impactful, ressonant, bolder. Drop boring qualifiers: instead of \"very important\" say \"crucial\". Remove any brakes and cushions. Finally, find the most relevant idea you want to deliver. Go back and add hints here and there to build towards it: drop an open question early on, or a subtle mention in a related passage. Repeat it a few times to make it ressonate. And that's it. When you're done with this third draft, your text is ready to publish. Put it out there and move on to the next piece. I purposefully left out anything concerning getting feedback, and not because it's irrelevant. On the contrary, getting feedback early on is critical. But this is a sufficiently complex topic on its own, so I'll leave it for another ocasion.","title":"Polish for impact"},{"location":"journal/","text":"The following is a cronological collection of random thoughts. There is no inherent structure, order, topic, or anything. It's just a braindump of whatever ocurred to me on those days where I had enough time to sit down and write. In time, if some kind of structure arises, I'll update this page with relevant links \ud83e\udd1e.","title":"Journal Index"},{"location":"journal/2021/08-24/","text":"Entry #1 - August 24th, 2021 \u00b6 Dear Journal: Is this even the right way to start a journal? Anyway. This is my Nth attempt at starting a journal. I hope this time it lasts. The purpose of this journal is to document my daily thoughts in a way that is less polished and thus takes less time than, say, a blog article or a full essay. So this is me, as unpolished as it gets (well, actually, since this is an electronic medium, I cannot help but comeback and fix some typos here and there, but I'm promissing myself not to consciously edit these words after typing them.) Why English, though? It is weird, but I find it easier to organize my thoughts if I have to think in English, perhaps because, not being my native language, it forces me to think harder. At the same time, it's an excuse to practice writing in English, which is around half of my daytime job. So, here I am embracing writing a journal in a foreign language, in the hope that the future me will not consider it stupid. So, on with the content! I've spent most of the day tweaking things here and there on a few projects. At one point, I decided to turn a piece of code in a side gig into a full-featured project in itself, but I stumbled against a wall. Long story short, a few weeks ago I was trying strapi for a project, but found it extremely convoluted, slow, and big for what I needed. So I decided to hack a poor man's headless CMS using just streamlit , fastapi , and MongoDB . And it worked! For the most part, at least. So, I figured, \"I should turn this into a Python module in itself!\" Haha, how naive of me. I created the GitHub repository, the README and LICENSE files, and then, when I was just about to start hacking, I stumbled again with the millenia old question: \"Which Python package manager should I use?\". The last time I succesfully started a Python project, it was with autogoal , and we used Poetry for that. And don't get me wrong, Poetry is probably as good as it gets in the Python build ecosystem, but it is still lacking. For starters, it's pretty slow, especially when dependencies become slightly big, and you're on a slightly slow connection (hello, Cuba, anyone?). Part of the reason is that Poetry sometimes, apparently more often than not, has to download a damn Python package just to inspect it's dependencies! So I've been hunting for an alternative solution for a while. Flirted with flit a little, but it doesn't do dependency resolution (at the moment of writing this at least). I also saw pyflow , and kind of made my mind, but then the FOMO started kicking really hard. I started thinking \"It's 2021 already, I must be missing something, there has to be a better way\". Thus, I navigated GitHub up and down looking for cookiecutter recipes for Python projects, and no, everything is still so 2020. And sadly, I came once again to the conclusion that, as there are N solutions for making Python projects, and none works for me, I'll have the invent the N+1! So now I'm giving myself some time to talk myself out of this idea. If I succeed, I'll probably give Poetry another shot. If I fail, then I'll probably try to hack together yet another Python project management workflow. I think it's gonna be pure Docker-based... Yes! That's it! You just clone, docker-compose up , and the project will be totally self-contained. You won't even need to have Python installed! This is gonna be the beginning of a great adventure! Or not. I guess the next few days will be decisive. I'll probably get back here and leave a note about whatever turns out from this idea !","title":"August 24"},{"location":"journal/2021/08-24/#entry-1-august-24th-2021","text":"Dear Journal: Is this even the right way to start a journal? Anyway. This is my Nth attempt at starting a journal. I hope this time it lasts. The purpose of this journal is to document my daily thoughts in a way that is less polished and thus takes less time than, say, a blog article or a full essay. So this is me, as unpolished as it gets (well, actually, since this is an electronic medium, I cannot help but comeback and fix some typos here and there, but I'm promissing myself not to consciously edit these words after typing them.) Why English, though? It is weird, but I find it easier to organize my thoughts if I have to think in English, perhaps because, not being my native language, it forces me to think harder. At the same time, it's an excuse to practice writing in English, which is around half of my daytime job. So, here I am embracing writing a journal in a foreign language, in the hope that the future me will not consider it stupid. So, on with the content! I've spent most of the day tweaking things here and there on a few projects. At one point, I decided to turn a piece of code in a side gig into a full-featured project in itself, but I stumbled against a wall. Long story short, a few weeks ago I was trying strapi for a project, but found it extremely convoluted, slow, and big for what I needed. So I decided to hack a poor man's headless CMS using just streamlit , fastapi , and MongoDB . And it worked! For the most part, at least. So, I figured, \"I should turn this into a Python module in itself!\" Haha, how naive of me. I created the GitHub repository, the README and LICENSE files, and then, when I was just about to start hacking, I stumbled again with the millenia old question: \"Which Python package manager should I use?\". The last time I succesfully started a Python project, it was with autogoal , and we used Poetry for that. And don't get me wrong, Poetry is probably as good as it gets in the Python build ecosystem, but it is still lacking. For starters, it's pretty slow, especially when dependencies become slightly big, and you're on a slightly slow connection (hello, Cuba, anyone?). Part of the reason is that Poetry sometimes, apparently more often than not, has to download a damn Python package just to inspect it's dependencies! So I've been hunting for an alternative solution for a while. Flirted with flit a little, but it doesn't do dependency resolution (at the moment of writing this at least). I also saw pyflow , and kind of made my mind, but then the FOMO started kicking really hard. I started thinking \"It's 2021 already, I must be missing something, there has to be a better way\". Thus, I navigated GitHub up and down looking for cookiecutter recipes for Python projects, and no, everything is still so 2020. And sadly, I came once again to the conclusion that, as there are N solutions for making Python projects, and none works for me, I'll have the invent the N+1! So now I'm giving myself some time to talk myself out of this idea. If I succeed, I'll probably give Poetry another shot. If I fail, then I'll probably try to hack together yet another Python project management workflow. I think it's gonna be pure Docker-based... Yes! That's it! You just clone, docker-compose up , and the project will be totally self-contained. You won't even need to have Python installed! This is gonna be the beginning of a great adventure! Or not. I guess the next few days will be decisive. I'll probably get back here and leave a note about whatever turns out from this idea !","title":"Entry #1 - August 24th, 2021"},{"location":"journal/2021/08-28/","text":"Entry #2 - August 28th, 2021 \u00b6 Hey, Journal: So, second entry, huh? Who would have thought? This is as far as I've ever gotten in a journal... The last three days I've mostly worked on pydock , which is my response to that concern I raised in the previous entry about the myriad of suboptimal solution for managing Python environments. My take is the N+1 suboptimal solution, but hey, at least it's mine, right? The idea is dead simple, so much so that I'm intrigued no one has tried it before (or maybe they did try but didn't survive to tell the story). pydock is a wrapper around Docker, that tries to give the impression of an environment manager, but under the hood is actually just creating Docker images and containers. The nicest part is that every \"environment\" is just a couple of files (a dockerfile and a requirements.txt ), out of which the whole environment can be deterministically rebuilt (because packages in requirements.txt are always pinned to exact versions). Anyway, I've been coding this thing for a couple days, on and off while doing other stuff, and it's reaching a point where I think I'll stick to it. This whole endeavour reminded me of the discussion about reinventing the wheel. We all know that's bad. Everytime you have to solve a problem that someone else already solved, you're putting yourself in the position of making the same mistakes they did and thus wasting a lot of precious time. But there are obvious advantages to reinventing wheels once in a while. For starters, it's very pedagogical. By redoing something that's well known, we give ourselves a chance to hone our skills in a safe environment, knowing that if we get stuck, there's plenty of guidance lying around. It's no wonder most education is about reinventing a bunch of wheels just to learn how they work. But that's not all. Retrying something that's apparently well solved might show us that, in reality, it wasn't that well solved after all. The thing is, times change, technology improves, problems mutate, and we are often stuck with solutions that were optimal for a previous version of reality, which is close, but not exactly like the one we're living. Since reinventing the wheel is such a tabu in most of engineering and science, we resort to incremental improvement. That is, we try to adjust the suboptimal solution with the minimal effort that makes it optimal again, now that the landscape has changed. It's like optimizing an slowly but ever-changing function by gradient descent with near infinitesimal steps: you're always converging to a local optimum that can be arbitrarily worse than the global optimum. So, from time to time, it's good to restart the search from scratch. Just take a look around and ask: \"What is a problem in my field that hasn't been solved from scratch in a while?\" And then try to solve that problem without thinking in terms of the previous solution. Just think again from first principles, and apply everything that the previous guy who solved it didn't have at hand. You'll probably end in a completely different region of the solution space. I'm not saying this is how all, or even most of innovation and science should be done. On te contrary, standing on the should of giants, doing incremental improvement, is a pretty solid way to move forward. But maybe 5% of the times looking at an old problem with a blank slate can be just what you need to do to come up with a beautifully innovative solution. Anyway, I started at Python environment managers and went down the rabbit hole of innovation and reinventing the wheel. I guess that's my way of saying: \"I think I'll reinvent this wheel and see how it goes.\" For the record, I'm not attempting to solve environment management in a better way to existing alternatives. For now, it's just a different way, one that fits my very specific needs; and if it helps others, so be it. I'll let you know how it goes.","title":"August 28"},{"location":"journal/2021/08-28/#entry-2-august-28th-2021","text":"Hey, Journal: So, second entry, huh? Who would have thought? This is as far as I've ever gotten in a journal... The last three days I've mostly worked on pydock , which is my response to that concern I raised in the previous entry about the myriad of suboptimal solution for managing Python environments. My take is the N+1 suboptimal solution, but hey, at least it's mine, right? The idea is dead simple, so much so that I'm intrigued no one has tried it before (or maybe they did try but didn't survive to tell the story). pydock is a wrapper around Docker, that tries to give the impression of an environment manager, but under the hood is actually just creating Docker images and containers. The nicest part is that every \"environment\" is just a couple of files (a dockerfile and a requirements.txt ), out of which the whole environment can be deterministically rebuilt (because packages in requirements.txt are always pinned to exact versions). Anyway, I've been coding this thing for a couple days, on and off while doing other stuff, and it's reaching a point where I think I'll stick to it. This whole endeavour reminded me of the discussion about reinventing the wheel. We all know that's bad. Everytime you have to solve a problem that someone else already solved, you're putting yourself in the position of making the same mistakes they did and thus wasting a lot of precious time. But there are obvious advantages to reinventing wheels once in a while. For starters, it's very pedagogical. By redoing something that's well known, we give ourselves a chance to hone our skills in a safe environment, knowing that if we get stuck, there's plenty of guidance lying around. It's no wonder most education is about reinventing a bunch of wheels just to learn how they work. But that's not all. Retrying something that's apparently well solved might show us that, in reality, it wasn't that well solved after all. The thing is, times change, technology improves, problems mutate, and we are often stuck with solutions that were optimal for a previous version of reality, which is close, but not exactly like the one we're living. Since reinventing the wheel is such a tabu in most of engineering and science, we resort to incremental improvement. That is, we try to adjust the suboptimal solution with the minimal effort that makes it optimal again, now that the landscape has changed. It's like optimizing an slowly but ever-changing function by gradient descent with near infinitesimal steps: you're always converging to a local optimum that can be arbitrarily worse than the global optimum. So, from time to time, it's good to restart the search from scratch. Just take a look around and ask: \"What is a problem in my field that hasn't been solved from scratch in a while?\" And then try to solve that problem without thinking in terms of the previous solution. Just think again from first principles, and apply everything that the previous guy who solved it didn't have at hand. You'll probably end in a completely different region of the solution space. I'm not saying this is how all, or even most of innovation and science should be done. On te contrary, standing on the should of giants, doing incremental improvement, is a pretty solid way to move forward. But maybe 5% of the times looking at an old problem with a blank slate can be just what you need to do to come up with a beautifully innovative solution. Anyway, I started at Python environment managers and went down the rabbit hole of innovation and reinventing the wheel. I guess that's my way of saying: \"I think I'll reinvent this wheel and see how it goes.\" For the record, I'm not attempting to solve environment management in a better way to existing alternatives. For now, it's just a different way, one that fits my very specific needs; and if it helps others, so be it. I'll let you know how it goes.","title":"Entry #2 - August 28th, 2021"},{"location":"newsletter/06-ssl/","text":"\ud83d\udd96 Welcome to another issue of the Mostly Harmless AI newsletter. Machine learning is the hottest topic in AI these days. One of the biggest challenges, though, is that it requires vast amounts of data. Finding good labelled data is very hard, and using unlabelled data is difficult. In this issue, we'll take a look at a promising machine learning technique that sits between these two extremes and promises to bring the best of both paradigms: self-supervised learning . \ud83d\uddde\ufe0f What's new \u00b6 Facebook recently released a blog post about their new project, Learning from Videos . One of these is SEER , a new vision model that beats state-of-the-art in several well-known benchmarks. These are all instances of the same underlying technique: self-supervised learning . What is it, and why is it hitting the news? https://twitter.com/facebookai/status/1370420003690983430 You can read a deep dive from Yann LeCun itself , but the TL;DR is this. Self-supervised learning (SSL) is a technique for leveraging vast amounts of unlabeled data by using the data's own structure as supervised labels. Let's unpack that. The most famous self-supervised models are probably modern Transformers: BERT, GPT, and company. They are trained on lots of lots of text, but, here is the key part: we don't labels. Instead, we take a sentence, hide some words, and train the model to predict the unknown words from the known ones. Pretty clever, right? By forcing the model to predict the missing bits, we are implicitly making it learn the underlying structure of the data. With text, it is pretty straightforward (well, after they tell you about it...) but with images is a whole different story. This is where Facebook's new SEER model enters the picture. It is a major breakthrough, akin to what BERT and similar models meant for NLP a few years ago. And the best part, they open-sourced it . \ud83d\udcda For learners \u00b6 If you're interested in learning more about self-supervised learning, check out this awesome Github list of resources. It has a large collection of papers and resources on self-supervised learning in several domains, from images to audio, to natural language processing. And very up-to-date. You can also take a look at PapersWithCode section on self-supervision , which collects some of the most relevant papers with links to their Github implementations. And finally, here's a not-so-recent talk from Yann LeCun that you'll enjoy on the topic. \ud83d\udee0\ufe0f Tools of the trade \u00b6 Probably the best all-in-one resource for self-supervised learning in computer vision is Facebook's VISSL project . It contains implementations of the most popular SSL models in Pytorch. If you prefer Tensorflow, here's Google implemenation of SimCLR , one of those state-of-the-art models. \ud83c\udf7f Recommendations \u00b6 Taking a step away from the hard technical topic, today I want to recommend one of the best sci-fi series in recent times, and probably the best modern take on the issue of artificial consciousness: Westworld . If you haven't seen it yet, I cannot recommend it enough. The ethical and philosophical topics are deeply treated, and the drama is top-notch, including some mind-blowing storyline twisting and turning that will leave you confused at times, but always impressed. \ud83c\udfa4 Word of mouth \u00b6 This week's AMA was not as crowded as usual, but also packed with interesting questions. We talked about Explainable AI , how to keep up with tech , the value of getting a Tensorflow certification , why not to do a PhD , and more. https://twitter.com/AlejandroPiad/status/1370777555360419840 And tying back to self-supervise learning, here's an interesting discussion in HackerNews about the challenges and potential issues with this paradigm. As usual, take it with a grain of salt. \ud83d\udc65 Community \u00b6 In this issue, I want to recommend you to follow these two Twitter accounts. They are still small (in Twitter numbers), but very productive, and I've enjoyed interacting with them both a lot this last few months: Tolani @JaiyeTikolo , and Dimas @DreamOnShadows . Give them both the chance to fill your timeline with interesting stuff all around AI and related tech. \u2615 Homebrew \u00b6 On my end, I've been working on a lot of stuff, including new scripts for podcast episodes. If you want to weigh in, I would love to get your opinion on what topics to touch on first. https://twitter.com/AlejandroPiad/status/1370435757094141957 Finally, I've cooked up a small and dirty script to schedule Twitter threads from Trello, using Python. Here's a short thread about it: https://twitter.com/AlejandroPiad/status/1370767653971881985 And here's the script . Feel free to remix it and reuse it as you see fit. It's 100% open-source code. \ud83d\udc4b That\u2019s it for now. Please let me know what do you think of this issue, what would you like to see more or less of, and any feedback you want to share. If you liked this newsletter, consider subscribing (in case you\u2019re not) and forwarding it to those you love. It\u2019s \ud83d\udcaf free!","title":"06 ssl"},{"location":"newsletter/06-ssl/#whats-new","text":"Facebook recently released a blog post about their new project, Learning from Videos . One of these is SEER , a new vision model that beats state-of-the-art in several well-known benchmarks. These are all instances of the same underlying technique: self-supervised learning . What is it, and why is it hitting the news? https://twitter.com/facebookai/status/1370420003690983430 You can read a deep dive from Yann LeCun itself , but the TL;DR is this. Self-supervised learning (SSL) is a technique for leveraging vast amounts of unlabeled data by using the data's own structure as supervised labels. Let's unpack that. The most famous self-supervised models are probably modern Transformers: BERT, GPT, and company. They are trained on lots of lots of text, but, here is the key part: we don't labels. Instead, we take a sentence, hide some words, and train the model to predict the unknown words from the known ones. Pretty clever, right? By forcing the model to predict the missing bits, we are implicitly making it learn the underlying structure of the data. With text, it is pretty straightforward (well, after they tell you about it...) but with images is a whole different story. This is where Facebook's new SEER model enters the picture. It is a major breakthrough, akin to what BERT and similar models meant for NLP a few years ago. And the best part, they open-sourced it .","title":"\ud83d\uddde\ufe0f What's new"},{"location":"newsletter/06-ssl/#for-learners","text":"If you're interested in learning more about self-supervised learning, check out this awesome Github list of resources. It has a large collection of papers and resources on self-supervised learning in several domains, from images to audio, to natural language processing. And very up-to-date. You can also take a look at PapersWithCode section on self-supervision , which collects some of the most relevant papers with links to their Github implementations. And finally, here's a not-so-recent talk from Yann LeCun that you'll enjoy on the topic.","title":"\ud83d\udcda For learners"},{"location":"newsletter/06-ssl/#tools-of-the-trade","text":"Probably the best all-in-one resource for self-supervised learning in computer vision is Facebook's VISSL project . It contains implementations of the most popular SSL models in Pytorch. If you prefer Tensorflow, here's Google implemenation of SimCLR , one of those state-of-the-art models.","title":"\ud83d\udee0\ufe0f Tools of the trade"},{"location":"newsletter/06-ssl/#recommendations","text":"Taking a step away from the hard technical topic, today I want to recommend one of the best sci-fi series in recent times, and probably the best modern take on the issue of artificial consciousness: Westworld . If you haven't seen it yet, I cannot recommend it enough. The ethical and philosophical topics are deeply treated, and the drama is top-notch, including some mind-blowing storyline twisting and turning that will leave you confused at times, but always impressed.","title":"\ud83c\udf7f Recommendations"},{"location":"newsletter/06-ssl/#word-of-mouth","text":"This week's AMA was not as crowded as usual, but also packed with interesting questions. We talked about Explainable AI , how to keep up with tech , the value of getting a Tensorflow certification , why not to do a PhD , and more. https://twitter.com/AlejandroPiad/status/1370777555360419840 And tying back to self-supervise learning, here's an interesting discussion in HackerNews about the challenges and potential issues with this paradigm. As usual, take it with a grain of salt.","title":"\ud83c\udfa4 Word of mouth"},{"location":"newsletter/06-ssl/#community","text":"In this issue, I want to recommend you to follow these two Twitter accounts. They are still small (in Twitter numbers), but very productive, and I've enjoyed interacting with them both a lot this last few months: Tolani @JaiyeTikolo , and Dimas @DreamOnShadows . Give them both the chance to fill your timeline with interesting stuff all around AI and related tech.","title":"\ud83d\udc65 Community"},{"location":"newsletter/06-ssl/#homebrew","text":"On my end, I've been working on a lot of stuff, including new scripts for podcast episodes. If you want to weigh in, I would love to get your opinion on what topics to touch on first. https://twitter.com/AlejandroPiad/status/1370435757094141957 Finally, I've cooked up a small and dirty script to schedule Twitter threads from Trello, using Python. Here's a short thread about it: https://twitter.com/AlejandroPiad/status/1370767653971881985 And here's the script . Feel free to remix it and reuse it as you see fit. It's 100% open-source code. \ud83d\udc4b That\u2019s it for now. Please let me know what do you think of this issue, what would you like to see more or less of, and any feedback you want to share. If you liked this newsletter, consider subscribing (in case you\u2019re not) and forwarding it to those you love. It\u2019s \ud83d\udcaf free!","title":"\u2615 Homebrew"},{"location":"newsletter/07-automl/","text":"\ud83d\udd96 Welcome to another issue of the Mostly Harmless AI newsletter. Machine learning is becoming more and more a fundamental component in the products and services we consume everyday. But the craft is still very much in its infancy, and remains a challenging topic for newcommers, especially those without a formal education. In this issue, we'll take a look at an emerging paradigm that promises to bring a revolution to the machine learning field, much like compilers where in the 70s and 80s: Automated Machine Learning . \ud83c\udf1f The topic \u00b6 Everyone who's done even a bit of machine learning has had to struggle with the intricacies of getting a model to work outside toy problems. There are literally hundreds if not thousands of knobs you can turn to make training more efficient, more robust, or even just to make it converge. Machine learning as a science has come very far in the last couple of decades, but as an engineering it is still in its infancy. Enter AutoML, or Automated Machine Learning. Sounds almost oxymoronic, or tautological: what does it even mean to automate machine learning? AutoML is an emerging subfield of machine learning that attempts to progresively optimize most of the process for solving a typical machine learning problem. At a very high level, we can think of this process as a series of steps such as data collection and preparation, feature engineering, model selection and tuning, deployment, monitoring, and so on. Each of these steps includes a lot of technical decisions with a myriad of options. Just take model selection: are we going full-in with neural networks, or should we try some of the basic models first? Which ones, for the matter? And don't get me started with neural networks, we have to pick among layers, activation functions, regularizers, optimizers... If we look at data preparation, again, there's a plethora of options: how to impute missing values (or whether to do it at all), how to encode the data, which filters to apply... You get the point. With AutoML we can reduce the cognitive load of these million decisions significantly, though it's no silver bullet, of course. One of the most successful subtasks in AutoML is model selection and tuning. Say you want to try all the classifiers in scikit-learn . You can loop through all the classes (fortunately they share a common API), but you also have to try different parameters in each class, whose valid value ranges are different for each class. And then each model has to be trained and cross-validated, how many times? Instead, you can use something like auto-sklearn , a library that wraps all scikit-learn models under a single class AutoClassifier . Jumping to the deep learning world, the model selection problem is often framed as Neural Architecture Search (NAS) . There are literally hundreds of different techniques but in the end it all boils down to the same idea: an intelligent search over the possible architectures of neural networks. The world of AutoML is already huge, and keeps growing. There are lots of open source libraries you can use today, and the big players in the industry are already packing their cloud platforms with these tools. Be aware, though, that the field is still in its infancy, and it's no free lunch. As AutoML is basically machine learning on steroids, all the existing challenges are multiplied. The cost of training an AutoML model is orders of magnitude larger than training a single model, although a lot of research is being put into reducing it. And two major issues that remain ahead are related to the explainability and the intrinsic biases encoded in these models. In the meantime, next time you have to train a somewhat conventional model on a somewhat conventional dataset, make yourself a favor and use of the many AutoML libraries available. You'll be surprised of the results and, even if you still need to tinker with details, you will at least start with a very good baseline to build upon. \ud83d\udcda For learners \u00b6 The best resource for getting up-to-date with the field is the AutoML.org website . You'll find links to papers, tools, and a very good introductory book . You can also check this awesome list on Github with links to hundreds of papers, books, tools, blog posts, slides, and more. \ud83d\udee0\ufe0f Tools of the trade \u00b6 It's impossible to list all the incredible AutoML libraries out there, so I'll share just two most-known that will get you pretty far. autosklearn is an AutoML wrapper for scikit-learn that gives you a black-box classifier. Under the hood, it's powered by Bayesian optimization, a super cool technique to efficiently explore large and complex spaces of parameters with a non-trivial structure. https://automl.github.io/auto-sklearn/master/ autokeras is keras -based AutoML framework. You'll find a few high-level models, like an image or a text classifier, which when fit perform a neural architecture search over a space of sensibly predefined architectures. https://autokeras.com/ auto-pytorch is a similar tools but for pytorch . You'll find a few high-level models, like an image or a text classifier, which when fit perform a neural architecture search over a space of sensibly predefined architectures. (Not all AutoML libraries are named following the regex pattern auto-?(\\w+) , but it's pretty common...) \ud83c\udf7f Recommendations \u00b6 The last couple of weeks I've been reading Stephen King's \"On Writing\" . It's both a short autobriography and a manual for writing better from the genius of the horror genre. Whether you like his books or not, you gotta concede he can write, in a way that makes many of his readers (myself include) incapable of not rendering in their minds his imaginations. If you want to know the secret sauce behind the success of his narrative art, and how to apply those same tools to your own writing, whether fiction or not, this book pretty much summarizes it. \ud83c\udfa4 Word of mouth \u00b6 Last Friday I challenged you to share with us an intriguing philosophical issue you couldn't stop thinking about. Lots of you answered, with mindblowing questions that sparked discussions still alive today. https://twitter.com/AlejandroPiad/status/1372850346243072001 I also submitted one such dilemma, a contrived scenario where a self-driven car would have to choose in an impossible situation between different equally bad outcomes. The purpose was to surface the need to think deeply about morality and how to encode it on our AI systems because, regardless of our technological advancement, we are still humans, and AI is reaching the point where it will inevitably have to deal with fundamental human issues. https://twitter.com/AlejandroPiad/status/1372880546381103227 \ud83d\udc65 Community \u00b6 This week I want you to follow three researchers from the team that created auto-sklearn . The have been doing a tremendous job organizing workshops, writing survey papers, and working on the basic science, to bring the field of AutoML into frontline research. https://twitter.com/KEggensperger https://twitter.com/FrankRHutter https://twitter.com/__mfeurer__ I was fortunate to participate in one of their workshops and meet them, and I can tell you they also happen to be geniunely good people, driven by the desire to make machine learning available to as many of us as possible. \u2615 Homebrew \u00b6 Since this issue is on the topic of AutoML, I want to tell you about yet another framework, but this time one I'm personally involved with. https://autogoal.github.io/ autogoal is a different approach to AutoML. Instead of wrapping an existing library with a transparent API (like auto-sklearn ) or give you high-level constructs (like autokeras ), we tried to mix and match the most common ML tools you already use under a single unified API. The library is a still in its infancy, but it's quickly growing. It currently include hundreds of algorithms from sklearn , spacy , gensim , nltk , keras , pytorch , that you can use as black-box machine learning models. The most interesting part, however, is that under the hood everything is powered by a very flexible DSL that you can adapt to any new library. It would mean to world to me if you could come to our Github and leave us star, or give us a follow at Twitter . \ud83d\udc4b That\u2019s it for now. Please let me know what do you think of this issue, what would you like to see more or less of, and any feedback you want to share. If you liked this newsletter, consider subscribing (in case you\u2019re not) and forwarding it to those you love. It\u2019s \ud83d\udcaf free!","title":"07 automl"},{"location":"newsletter/07-automl/#the-topic","text":"Everyone who's done even a bit of machine learning has had to struggle with the intricacies of getting a model to work outside toy problems. There are literally hundreds if not thousands of knobs you can turn to make training more efficient, more robust, or even just to make it converge. Machine learning as a science has come very far in the last couple of decades, but as an engineering it is still in its infancy. Enter AutoML, or Automated Machine Learning. Sounds almost oxymoronic, or tautological: what does it even mean to automate machine learning? AutoML is an emerging subfield of machine learning that attempts to progresively optimize most of the process for solving a typical machine learning problem. At a very high level, we can think of this process as a series of steps such as data collection and preparation, feature engineering, model selection and tuning, deployment, monitoring, and so on. Each of these steps includes a lot of technical decisions with a myriad of options. Just take model selection: are we going full-in with neural networks, or should we try some of the basic models first? Which ones, for the matter? And don't get me started with neural networks, we have to pick among layers, activation functions, regularizers, optimizers... If we look at data preparation, again, there's a plethora of options: how to impute missing values (or whether to do it at all), how to encode the data, which filters to apply... You get the point. With AutoML we can reduce the cognitive load of these million decisions significantly, though it's no silver bullet, of course. One of the most successful subtasks in AutoML is model selection and tuning. Say you want to try all the classifiers in scikit-learn . You can loop through all the classes (fortunately they share a common API), but you also have to try different parameters in each class, whose valid value ranges are different for each class. And then each model has to be trained and cross-validated, how many times? Instead, you can use something like auto-sklearn , a library that wraps all scikit-learn models under a single class AutoClassifier . Jumping to the deep learning world, the model selection problem is often framed as Neural Architecture Search (NAS) . There are literally hundreds of different techniques but in the end it all boils down to the same idea: an intelligent search over the possible architectures of neural networks. The world of AutoML is already huge, and keeps growing. There are lots of open source libraries you can use today, and the big players in the industry are already packing their cloud platforms with these tools. Be aware, though, that the field is still in its infancy, and it's no free lunch. As AutoML is basically machine learning on steroids, all the existing challenges are multiplied. The cost of training an AutoML model is orders of magnitude larger than training a single model, although a lot of research is being put into reducing it. And two major issues that remain ahead are related to the explainability and the intrinsic biases encoded in these models. In the meantime, next time you have to train a somewhat conventional model on a somewhat conventional dataset, make yourself a favor and use of the many AutoML libraries available. You'll be surprised of the results and, even if you still need to tinker with details, you will at least start with a very good baseline to build upon.","title":"\ud83c\udf1f The topic"},{"location":"newsletter/07-automl/#for-learners","text":"The best resource for getting up-to-date with the field is the AutoML.org website . You'll find links to papers, tools, and a very good introductory book . You can also check this awesome list on Github with links to hundreds of papers, books, tools, blog posts, slides, and more.","title":"\ud83d\udcda For learners"},{"location":"newsletter/07-automl/#tools-of-the-trade","text":"It's impossible to list all the incredible AutoML libraries out there, so I'll share just two most-known that will get you pretty far. autosklearn is an AutoML wrapper for scikit-learn that gives you a black-box classifier. Under the hood, it's powered by Bayesian optimization, a super cool technique to efficiently explore large and complex spaces of parameters with a non-trivial structure. https://automl.github.io/auto-sklearn/master/ autokeras is keras -based AutoML framework. You'll find a few high-level models, like an image or a text classifier, which when fit perform a neural architecture search over a space of sensibly predefined architectures. https://autokeras.com/ auto-pytorch is a similar tools but for pytorch . You'll find a few high-level models, like an image or a text classifier, which when fit perform a neural architecture search over a space of sensibly predefined architectures. (Not all AutoML libraries are named following the regex pattern auto-?(\\w+) , but it's pretty common...)","title":"\ud83d\udee0\ufe0f Tools of the trade"},{"location":"newsletter/07-automl/#recommendations","text":"The last couple of weeks I've been reading Stephen King's \"On Writing\" . It's both a short autobriography and a manual for writing better from the genius of the horror genre. Whether you like his books or not, you gotta concede he can write, in a way that makes many of his readers (myself include) incapable of not rendering in their minds his imaginations. If you want to know the secret sauce behind the success of his narrative art, and how to apply those same tools to your own writing, whether fiction or not, this book pretty much summarizes it.","title":"\ud83c\udf7f Recommendations"},{"location":"newsletter/07-automl/#word-of-mouth","text":"Last Friday I challenged you to share with us an intriguing philosophical issue you couldn't stop thinking about. Lots of you answered, with mindblowing questions that sparked discussions still alive today. https://twitter.com/AlejandroPiad/status/1372850346243072001 I also submitted one such dilemma, a contrived scenario where a self-driven car would have to choose in an impossible situation between different equally bad outcomes. The purpose was to surface the need to think deeply about morality and how to encode it on our AI systems because, regardless of our technological advancement, we are still humans, and AI is reaching the point where it will inevitably have to deal with fundamental human issues. https://twitter.com/AlejandroPiad/status/1372880546381103227","title":"\ud83c\udfa4 Word of mouth"},{"location":"newsletter/07-automl/#community","text":"This week I want you to follow three researchers from the team that created auto-sklearn . The have been doing a tremendous job organizing workshops, writing survey papers, and working on the basic science, to bring the field of AutoML into frontline research. https://twitter.com/KEggensperger https://twitter.com/FrankRHutter https://twitter.com/__mfeurer__ I was fortunate to participate in one of their workshops and meet them, and I can tell you they also happen to be geniunely good people, driven by the desire to make machine learning available to as many of us as possible.","title":"\ud83d\udc65 Community"},{"location":"newsletter/07-automl/#homebrew","text":"Since this issue is on the topic of AutoML, I want to tell you about yet another framework, but this time one I'm personally involved with. https://autogoal.github.io/ autogoal is a different approach to AutoML. Instead of wrapping an existing library with a transparent API (like auto-sklearn ) or give you high-level constructs (like autokeras ), we tried to mix and match the most common ML tools you already use under a single unified API. The library is a still in its infancy, but it's quickly growing. It currently include hundreds of algorithms from sklearn , spacy , gensim , nltk , keras , pytorch , that you can use as black-box machine learning models. The most interesting part, however, is that under the hood everything is powered by a very flexible DSL that you can adapt to any new library. It would mean to world to me if you could come to our Github and leave us star, or give us a follow at Twitter . \ud83d\udc4b That\u2019s it for now. Please let me know what do you think of this issue, what would you like to see more or less of, and any feedback you want to share. If you liked this newsletter, consider subscribing (in case you\u2019re not) and forwarding it to those you love. It\u2019s \ud83d\udcaf free!","title":"\u2615 Homebrew"},{"location":"newsletter/08-languagemodels/","text":"\ud83d\udd96 Welcome to another issue of the Mostly Harmless AI newsletter. It's been a while since the last issue, and the reason is I've been struggling with finding the motivation to write something worth of sharing. Don't get me wrong, I have a ton of ideas (even I dare to say, one or two good ones), but most of them are better shaped for Twitter threads. Plus, there are already LOTS of great newsletters out there, both for long-form and short-form topics. So I decided that if I wanted to give a shot to this newsletter, it had to be something slightly more personal, something that only could come from me. Thus, I will be focusing more on sharing my journey, the things I'm working on, the problems I'm concerned about. Of course, all of this is tightly related to AI, which remains to be my primary (well, secondary) love. I hope these topics are something you find useful or at least mildly interesting. Most of it is mostly harmless, anyway. \ud83e\udd2c Language models are full of biases \u00b6 A couple threads ago I talked about computational language models. These are, in a nutshell, compressed representations of human language that assign a likelihood to every possible sentence. As a black box, you can imagine a language model as some kind of Python function that receives a sentence and outputs a number from 0 to 1, the higher the most likely that sentence actually \"exists\". They are used anywhere we need a computer to deal with natural language: automatic translation, speech-to-text, search engines, There are many ways to implement something like this, and you can take a look at this thread for some ideas: https://apiad.net/tweetstorms/mindblowingmonday/languagemodels Anyway, what I wanted to talk now is not that much about technical details, but rather about some problems that arise from the deployment of huge language models by big tech companies. You see, language models are often trained in an unsupervised (or self-supervised) form, fed with massive chunks of text mined from the Internet. This is a very cool idea in principle, because we have access to a vast collection of human language where basically everything we know about can be found. GPT-3 and BERT are just two examples of very different language models trained of huge amounts of text (they are in completely different leagues, though, in terms of training data). So, if you ask one of these language model the probability of a sentence like \"Leonardo da Vinci painted the Mona Lisa\" it should give it a very high score. However, if you ask it \"Alejandro Piad painted the Mona Lisa\", the score should be close to zero. The reason is very simple, there are far more examples of the first sentence than the second in the Internet. The model doesn't really know who painted the Mona Lisa, it just knows that many more people think it was Leonardo (keep in mind, though, that both you and me also reason like this a lot of times...) Now, if only the Internet was a place where all that's true was massively more common that what's false. But it isn't. It is full with conspiracy theories and fake news. So we must be careful in using frequency of something appearing in the Internet as a proxy for truthfulness. The big problem, though, comes not from purposefully misleading stuff, but from the subtle biases that creep into all of our conversations. For example, what happens when you ask a language model \"He is a programmer\" vs \"She is a programmer\"? Naturally, both sentences should be exactly equal in terms of likelihood. But a carelessly trained LM will very likely give a higher score to the first one. Why? Because the Internet has many more examples of programmer boys than girls! Why does this matter? In some applications this kind of biases pop up immediately. For example, if you Google translate a long paragraph including \"she is a programmer\" back and forth between English and a language without gender you can get \"he is a programmer\" back. But these are not the worst cases. You can use a language model with these biases as an internal component of another system, say, to evaluate candidates for job applications, or to assess the reliability of a legal claim, or to estimate if a person will forfeit a mortage, or to pre-screen papers submitted to a research journal. In these cases, you may have no idea how these biases are messing with the final prediction. As a very simplistic example, you could be rejecting women applying for programming jobs more often than men because their profile has less \"fit\" with the job description. So here comes the mandatory discussion about \"but that's the real data!\". Yes, it is. And that doesn't make it right. Reality is full of biases, full of wrong decisions, full of things we want to change. Letting those things creep into our models of reality unnoticed is a recipe for keeping ourselves in the place we are today, not in the place we want to be. Now, there's light at the end of the tunnel. Ethics and fairness is a big issue in the AI research community today. The most brilliant minds in our field our working in the detection and mitigation of these problems. The solution is not to vilify and stop using these technologies altogether. Language models are a very powerful tech that can boost some of the most interesting and useful applications of the next decade. The solution is to understand their limitations and deploy them with the necessary care in those scenarios where they're most likely to cause harm. \ud83d\udcda For learners \u00b6 If you want to learn more about some of the biggest issues in AI ethics today, there is a wonderful book, The Alignment Problem , by Brian Christian . It goes way beyond language biases, into the realm of reinforcement learning and the value alignment problem. \ud83d\udee0\ufe0f Tools of the trade \u00b6 If what you're looking for is to play with pre-trained language models, use them in your own app, or fine-tune them in your own dataset, then what you want is the transformers library by huggingface.co . \ud83d\udc65 Cool people to meet \u00b6 This week I want to recommend you to follow Talia . She's working on some of the coolest research in the intersection of programming languages and software engineering. Plus, she is incredibly energetic and overall a very nice person to talk with about some of the most difficult topics we are facing today. Another cool friend I made recently is Prashant . We've been talking a lot about some of the most intriguing philosophical questions around AI, consciousness, free will, you mention it. He's very active on Twitter and he shares a lot of interesting bits and resources about machine learning. Plus, he loves books! \u2615 Homebrew \u00b6 Finally, this week I want to share with you a small project I did a few months ago, auditorium . It's a slideshow generator based on the awesome reveal.js , which adds a Pythonic layer with which you can craft cool interactive slideshows with pure Python code. It's a bit rough around the edges, though, so it would be very cool if you could take it for a spin and let me know what you think of it! \ud83d\udc4b That\u2019s it for now. Please let me know what do you think of this issue, what would you like to see more or less of, and any feedback you want to share. If you liked this newsletter, consider subscribing (in case you\u2019re not) and forwarding it to those you love. It\u2019s \ud83d\udcaf free!","title":"08 languagemodels"},{"location":"newsletter/08-languagemodels/#language-models-are-full-of-biases","text":"A couple threads ago I talked about computational language models. These are, in a nutshell, compressed representations of human language that assign a likelihood to every possible sentence. As a black box, you can imagine a language model as some kind of Python function that receives a sentence and outputs a number from 0 to 1, the higher the most likely that sentence actually \"exists\". They are used anywhere we need a computer to deal with natural language: automatic translation, speech-to-text, search engines, There are many ways to implement something like this, and you can take a look at this thread for some ideas: https://apiad.net/tweetstorms/mindblowingmonday/languagemodels Anyway, what I wanted to talk now is not that much about technical details, but rather about some problems that arise from the deployment of huge language models by big tech companies. You see, language models are often trained in an unsupervised (or self-supervised) form, fed with massive chunks of text mined from the Internet. This is a very cool idea in principle, because we have access to a vast collection of human language where basically everything we know about can be found. GPT-3 and BERT are just two examples of very different language models trained of huge amounts of text (they are in completely different leagues, though, in terms of training data). So, if you ask one of these language model the probability of a sentence like \"Leonardo da Vinci painted the Mona Lisa\" it should give it a very high score. However, if you ask it \"Alejandro Piad painted the Mona Lisa\", the score should be close to zero. The reason is very simple, there are far more examples of the first sentence than the second in the Internet. The model doesn't really know who painted the Mona Lisa, it just knows that many more people think it was Leonardo (keep in mind, though, that both you and me also reason like this a lot of times...) Now, if only the Internet was a place where all that's true was massively more common that what's false. But it isn't. It is full with conspiracy theories and fake news. So we must be careful in using frequency of something appearing in the Internet as a proxy for truthfulness. The big problem, though, comes not from purposefully misleading stuff, but from the subtle biases that creep into all of our conversations. For example, what happens when you ask a language model \"He is a programmer\" vs \"She is a programmer\"? Naturally, both sentences should be exactly equal in terms of likelihood. But a carelessly trained LM will very likely give a higher score to the first one. Why? Because the Internet has many more examples of programmer boys than girls! Why does this matter? In some applications this kind of biases pop up immediately. For example, if you Google translate a long paragraph including \"she is a programmer\" back and forth between English and a language without gender you can get \"he is a programmer\" back. But these are not the worst cases. You can use a language model with these biases as an internal component of another system, say, to evaluate candidates for job applications, or to assess the reliability of a legal claim, or to estimate if a person will forfeit a mortage, or to pre-screen papers submitted to a research journal. In these cases, you may have no idea how these biases are messing with the final prediction. As a very simplistic example, you could be rejecting women applying for programming jobs more often than men because their profile has less \"fit\" with the job description. So here comes the mandatory discussion about \"but that's the real data!\". Yes, it is. And that doesn't make it right. Reality is full of biases, full of wrong decisions, full of things we want to change. Letting those things creep into our models of reality unnoticed is a recipe for keeping ourselves in the place we are today, not in the place we want to be. Now, there's light at the end of the tunnel. Ethics and fairness is a big issue in the AI research community today. The most brilliant minds in our field our working in the detection and mitigation of these problems. The solution is not to vilify and stop using these technologies altogether. Language models are a very powerful tech that can boost some of the most interesting and useful applications of the next decade. The solution is to understand their limitations and deploy them with the necessary care in those scenarios where they're most likely to cause harm.","title":"\ud83e\udd2c Language models are full of biases"},{"location":"newsletter/08-languagemodels/#for-learners","text":"If you want to learn more about some of the biggest issues in AI ethics today, there is a wonderful book, The Alignment Problem , by Brian Christian . It goes way beyond language biases, into the realm of reinforcement learning and the value alignment problem.","title":"\ud83d\udcda For learners"},{"location":"newsletter/08-languagemodels/#tools-of-the-trade","text":"If what you're looking for is to play with pre-trained language models, use them in your own app, or fine-tune them in your own dataset, then what you want is the transformers library by huggingface.co .","title":"\ud83d\udee0\ufe0f Tools of the trade"},{"location":"newsletter/08-languagemodels/#cool-people-to-meet","text":"This week I want to recommend you to follow Talia . She's working on some of the coolest research in the intersection of programming languages and software engineering. Plus, she is incredibly energetic and overall a very nice person to talk with about some of the most difficult topics we are facing today. Another cool friend I made recently is Prashant . We've been talking a lot about some of the most intriguing philosophical questions around AI, consciousness, free will, you mention it. He's very active on Twitter and he shares a lot of interesting bits and resources about machine learning. Plus, he loves books!","title":"\ud83d\udc65 Cool people to meet"},{"location":"newsletter/08-languagemodels/#homebrew","text":"Finally, this week I want to share with you a small project I did a few months ago, auditorium . It's a slideshow generator based on the awesome reveal.js , which adds a Pythonic layer with which you can craft cool interactive slideshows with pure Python code. It's a bit rough around the edges, though, so it would be very cool if you could take it for a spin and let me know what you think of it! \ud83d\udc4b That\u2019s it for now. Please let me know what do you think of this issue, what would you like to see more or less of, and any feedback you want to share. If you liked this newsletter, consider subscribing (in case you\u2019re not) and forwarding it to those you love. It\u2019s \ud83d\udcaf free!","title":"\u2615 Homebrew"},{"location":"podcast/","text":"Welcome to the Mostly Harmless AI podcast. In this podcast I'll share with you interesting bits about artificial intelligence, what it is, how it works, and why you should care about it. We'll talk a lot about the most exciting and novel discoveries. We will turn to history sometimes, and share some funny, enlightening, or emotive moments. And, we will discuss about the implications that these technologies can have on your daily life, and how can you make informed decisions on your interaction with AI systems today. You know, mostly harmless discussions.","title":"Index"},{"location":"podcast/what-is-ai/","text":"An origin story for Artificial Intelligence \u00b6 \ud83d\udd09 Listen on Anchor.fm Intro \u00b6 Artificial Intelligence stands at an intriguing crossroad between math, engineering, and philosophy. At its core, it's all about trying to answer some of the most transcendental questions: what is intelligence, what takes for a system to be able to sustain it, and whether can we build such a system. What is Artificial Intelligence about? To try and answer this question, let's start at the beginning, or at a beginning because, like every good story, this one has many versions. In this episode, I want to tell you an origin story for Artificial Intelligence. Is the story of how the dream of a fearless man revolutionized our comprehension of what it means to be, after all, human. Welcome to Mostly Harmless AI. The Imitation Game \u00b6 Our origin story starts in 1950, England, home of Alan Turing. Turing is very well known for at least three different things, each of which would independently be enough to warrant him a major place in the history of Computer Science. When taken together, his contributions make him, in the eyes of many, the most relevant figure in the whole field. His first major contribution to Computer Science was the definition of a mathematical model for an abstract kind of machine that could potentially perform any sort of computation. He basically defined the minimum requirements to make a computer. This model is now known by the very creative name of \"Turing machine\", and it lays the theoretical foundations of what can and, more importantly, what cannot be done with a computer, regardless of how powerful technology ever becomes. Had he ended there, he would have been remembered as the theoretical father of Computer Science. But he went further. During World War 2, he worked on a super-secret project to decipher Nazi cryptography. It was supposed to be uncrackable, but Turing teamed-up with some of the smartest people he could find, and they cracked it. And in doing so, they also built the first physical embodiment of an actual Turing machine, the first real working computer. So, he both created the foundational theory of the field and engineered the first computer. And then he turned his attention to what he considered was the ultimate question of this new science: can these machines ever think? In \"Computer Machinery and Intelligence\", a short technical paper written in 1950, Turing described what came to be known as the Turing Test, a hypothetical experiment to determine whether artificial intelligence could be considered, indeed, intelligent. He called it \"The Imitation Game\". The basic idea is something like this: you put both a human and a computer behind closed doors, from where they can communicate with another human, a judge, by a text interface only, like in a chat. The judge's job is to determine who is the human and who is the computer, and both interviews will do their best to convince the judge that they are human. So the real human can write whatever he or she wants: \"The human is me\", \"Don't trust the other\", anything. But here is the thing, the computer can also do the same, so in a sense, it has to deceive another human to be considered \"intelligent\". The judge can ask questions to trick the computer to reveal itself. Maybe ask it to solve some complex mathematical problem, something that no human could achieve. But the computer can just say \"hey, that's impossible to do!\", just like any human would. Turing thought the only way for a computer to successfully convince any potential judge, no matter how tricky the questions asked, would be to display such a wide range of creative and human-like responses that we would have to agree, for all practical means, it was indeed showing intelligence. The Turing Test gives us one possible definition for Artificial Intelligence: a machine that displays human-like responses in every conceivable conversation. It's kind of a tautological answer to the question of \"what is intelligence\". It's intelligence if you cannot effectively differentiate it from other things that you agree to call intelligence. There are a lot of issues with this definition, though, and we'll examine a few of them next. The issue of human biases \u00b6 First, we have to say that Turing conceived this test as a thought experiment, a hypothetical setup to force us into thinking about how can we even start to define what intelligence is about. Turing's idea was not that we would actually implement this test as he described it. It's more a philosophical definition of intelligence than a pragmatic one. Despite this, we did take his words at face value and implemented the test. The Loebner contest is probably the most famous one. It's a yearly contest in which teams of programmers submit chatbots that are evaluated against real humans in a very similar setting to what Turing proposed. One pragmatic argument against Turing's Test is that it relies heavily on the ability of a human to accurately judge whether those responses seem human-like, which is a very subjective thing to do. And we know for a fact that humans are lousy at subjectivity. We are full of biases, one of which is precisely our tendency to \"anthropomorphize\", that is, to see human features in non-human things. We do it with our pets, with inanimate objects, with symbols, ..., we even do it with characters in videogames! So of course we could anthropomorphize the computer behind that text message and assign it a higher degree of intelligence that it really has. This has happened in the Loebner contest over and over. Programmers design a chatbot that makes spelling mistakes on purpose and justifies itself as a non-native speaker to avoid answering complex questions. These techniques have allowed systems that are, for whatever definition of intelligence you have, definitely not intelligent, to win over 30% of the times. The Turing test, at least implemented in this simplistic scenario in the Loebner contest, seems to be very fragile to human biases. The issue of semantics \u00b6 Beyond practical or methodological problems, some thinkers have argued that there are fundamental issues with this definition of intelligence. Probably the most famous argument is \"The Chinese Room\", posed by philosopher John Searle. It's a thought counter-experiment in which a human is placed inside a room with a book that contains instructions for translating messages from Chinese to English, messages that enter the room through a small window. The man, who speaks nothing of Chinese, reads the message, finds the symbols in the book, and writes the corresponding answer. If the book is written in such a way that for every incoming message it will correctly produce the correct translation, any external observer would be convinced the \"room\" speaks Chinese, while it is obvious, according to Searle, that neither the man nor the book actually understands Chinese. According to Searle, this thought experiment shows that a system can display the ability to solve a cognitive task without actually possessing the intelligence necessary to do it. It's supposed to show that imitation of intelligence is not the same as actual intelligence. Some have argued, though, that even if neither the man nor the book understands Chinese, there is a sense in which the whole system, the room with all its content, does understand Chinese. It is the same sense in which Turing's test defines intelligence: if the room (with man and book inside) is indeed able to provide a perfectly plausible translation for any incoming message, what else do we need to convince ourselves it does understand Chinese? The point is, this kind of argument rests on agreeing on the definition of what \"knowing\" or \"understanding\" means. If your definition of \"understanding\" is such that only humans are capable of doing it, then, by your own definition, you cannot believe in Artificial Intelligence. Turing was a functionalist in this regard. He believed that intelligence is best defined in terms of what it can do, regardless of, for example, how is it made or what is it composed of. In his view, intelligence is being capable of maintaining a coherent conversation about any general topic, independently of the hardware (electronic or biological) that sustains that intelligence. You might disagree, and lots of very smart people do. Some believe intelligence has to be biological in nature. Others, that there is some layer of unknown physics at play inside our brains that cannot be simulated in a classical computer. Turing believed the brain was just a very powerful computer, and thus, in principle, nothing can stop us from eventually making an electronic substitute capable of harbouring intelligence. The issue of purpose \u00b6 Even if we agree philosophically with this definition of intelligence, it is still problematic in another sense. If we define intelligence as something at the level of what humans can currently do, isn't it kind of pointless to try and develop that? We already have humans, why do we need another system with the same capabilities? Wouldn't it be better if artificial intelligence can solve the problems that we humans cannot solve today? An argument in favour of Turing could be that his definition imposes no limitation to what this intelligence can do. It need not be as dumb as a human. In fact, a super-intelligent being should be perfectly capable of passing the test, the same way an adult can play with a child without actually believing in fairies. In a sense, we could say that being able to succeed at another human's judgement of character is the ultimate intelligence test because intelligence evolved precisely in this context: we were a bunch of primates in the savannah who needed to understand and trust each other to survive. But this anthropocentric view is not without issues as well. Who's to say that human intelligence is not a dead end? Couldn't we, by trying to imitate us, humans, be actually taking a detour on the path to superintelligence? Maybe our biases are just the evidence that, if there is higher intelligence in the universe, it is not like us. But if that's the case, could we even be able to recognize that intelligence? And could they, or it, recognize us? Turing's Test is far from perfect. It has practical, philosophical and ethical implications regarding how we define intelligence. In a sense, it is also about how we define ourselves since intelligence is arguable the human trait we are the proudest of. How we choose to define it also dictates how we attempt to imitate it, and how we go about recognizing it in others. Our biases are an intrinsic part of our intelligence, whatever that is. Conclusions? \u00b6 Turing's imitation game is the first attempt to formally define an overarching goal for artificial intelligence and the then nascent science of computation. However, this goal wasn't set in stone. In the next decades, different paradigms emerged, some of them more focused on what we today call general, or strong, AI, and others more focused on narrow, or weak, AI. The first group of paradigms attempts to solve AI more or less in the sense that Turing envisioned. To create an artificial intelligence so powerful that it can perform any cognitive task a human can, and then some we can't. The second attempts to solve concrete problems, such as vision, or language, or driving a car, that seem to require advanced levels of cognition but are not entirely general. For most of the history of AI, the narrow approaches have been the most successful. We are today capable of detecting tons of objects in images; translate between most mainstream languages; and play chess, Go, and even StarCraft better than any human that's ever lived. However, we are still very, very far from Turing's vision of achieving near-human capabilities at open-ended conversation. In a sense, we have walked a very long road, and yet we are still at the very beginning of this journey. And despite the dominating pragmatism, and the focus on solving narrow tasks, if you ask most of the people working today in the field, they will tell you that, deep down, they also share Turing's dream. Turing's life ended sadly. He was a gay man in a terribly unforgiving society, and he suffered physical and psychological humiliation for being unwilling to fit the narrow definition of human that others had decided. His greatest achievement was the dream he left us to pursue. A dream of a future in which humanity is no longer alone in the cognitive universe. A future we will share with our intellectual children, to whom we will show the marvels of the universe, and they will, in turn, help us unravel its deepest mysteries. Outro \u00b6 Thank you for listening. If you enjoyed this episode, feel free to leave a review, and share it with your loved ones. If you have any questions or suggestions, I would love to hear from you. You can find me on Twitter, and if you're listening on the Anchor app, you can leave a voice message right here. This was the Mostly Harmless AI podcast. I'll be back very soon with another episode on the fascinating world of Artificial Intelligence. Until then, stay curious, and stay safe...","title":"An origin story for Artificial Intelligence"},{"location":"podcast/what-is-ai/#an-origin-story-for-artificial-intelligence","text":"\ud83d\udd09 Listen on Anchor.fm","title":"An origin story for Artificial Intelligence"},{"location":"podcast/what-is-ai/#intro","text":"Artificial Intelligence stands at an intriguing crossroad between math, engineering, and philosophy. At its core, it's all about trying to answer some of the most transcendental questions: what is intelligence, what takes for a system to be able to sustain it, and whether can we build such a system. What is Artificial Intelligence about? To try and answer this question, let's start at the beginning, or at a beginning because, like every good story, this one has many versions. In this episode, I want to tell you an origin story for Artificial Intelligence. Is the story of how the dream of a fearless man revolutionized our comprehension of what it means to be, after all, human. Welcome to Mostly Harmless AI.","title":"Intro"},{"location":"podcast/what-is-ai/#the-imitation-game","text":"Our origin story starts in 1950, England, home of Alan Turing. Turing is very well known for at least three different things, each of which would independently be enough to warrant him a major place in the history of Computer Science. When taken together, his contributions make him, in the eyes of many, the most relevant figure in the whole field. His first major contribution to Computer Science was the definition of a mathematical model for an abstract kind of machine that could potentially perform any sort of computation. He basically defined the minimum requirements to make a computer. This model is now known by the very creative name of \"Turing machine\", and it lays the theoretical foundations of what can and, more importantly, what cannot be done with a computer, regardless of how powerful technology ever becomes. Had he ended there, he would have been remembered as the theoretical father of Computer Science. But he went further. During World War 2, he worked on a super-secret project to decipher Nazi cryptography. It was supposed to be uncrackable, but Turing teamed-up with some of the smartest people he could find, and they cracked it. And in doing so, they also built the first physical embodiment of an actual Turing machine, the first real working computer. So, he both created the foundational theory of the field and engineered the first computer. And then he turned his attention to what he considered was the ultimate question of this new science: can these machines ever think? In \"Computer Machinery and Intelligence\", a short technical paper written in 1950, Turing described what came to be known as the Turing Test, a hypothetical experiment to determine whether artificial intelligence could be considered, indeed, intelligent. He called it \"The Imitation Game\". The basic idea is something like this: you put both a human and a computer behind closed doors, from where they can communicate with another human, a judge, by a text interface only, like in a chat. The judge's job is to determine who is the human and who is the computer, and both interviews will do their best to convince the judge that they are human. So the real human can write whatever he or she wants: \"The human is me\", \"Don't trust the other\", anything. But here is the thing, the computer can also do the same, so in a sense, it has to deceive another human to be considered \"intelligent\". The judge can ask questions to trick the computer to reveal itself. Maybe ask it to solve some complex mathematical problem, something that no human could achieve. But the computer can just say \"hey, that's impossible to do!\", just like any human would. Turing thought the only way for a computer to successfully convince any potential judge, no matter how tricky the questions asked, would be to display such a wide range of creative and human-like responses that we would have to agree, for all practical means, it was indeed showing intelligence. The Turing Test gives us one possible definition for Artificial Intelligence: a machine that displays human-like responses in every conceivable conversation. It's kind of a tautological answer to the question of \"what is intelligence\". It's intelligence if you cannot effectively differentiate it from other things that you agree to call intelligence. There are a lot of issues with this definition, though, and we'll examine a few of them next.","title":"The Imitation Game"},{"location":"podcast/what-is-ai/#the-issue-of-human-biases","text":"First, we have to say that Turing conceived this test as a thought experiment, a hypothetical setup to force us into thinking about how can we even start to define what intelligence is about. Turing's idea was not that we would actually implement this test as he described it. It's more a philosophical definition of intelligence than a pragmatic one. Despite this, we did take his words at face value and implemented the test. The Loebner contest is probably the most famous one. It's a yearly contest in which teams of programmers submit chatbots that are evaluated against real humans in a very similar setting to what Turing proposed. One pragmatic argument against Turing's Test is that it relies heavily on the ability of a human to accurately judge whether those responses seem human-like, which is a very subjective thing to do. And we know for a fact that humans are lousy at subjectivity. We are full of biases, one of which is precisely our tendency to \"anthropomorphize\", that is, to see human features in non-human things. We do it with our pets, with inanimate objects, with symbols, ..., we even do it with characters in videogames! So of course we could anthropomorphize the computer behind that text message and assign it a higher degree of intelligence that it really has. This has happened in the Loebner contest over and over. Programmers design a chatbot that makes spelling mistakes on purpose and justifies itself as a non-native speaker to avoid answering complex questions. These techniques have allowed systems that are, for whatever definition of intelligence you have, definitely not intelligent, to win over 30% of the times. The Turing test, at least implemented in this simplistic scenario in the Loebner contest, seems to be very fragile to human biases.","title":"The issue of human biases"},{"location":"podcast/what-is-ai/#the-issue-of-semantics","text":"Beyond practical or methodological problems, some thinkers have argued that there are fundamental issues with this definition of intelligence. Probably the most famous argument is \"The Chinese Room\", posed by philosopher John Searle. It's a thought counter-experiment in which a human is placed inside a room with a book that contains instructions for translating messages from Chinese to English, messages that enter the room through a small window. The man, who speaks nothing of Chinese, reads the message, finds the symbols in the book, and writes the corresponding answer. If the book is written in such a way that for every incoming message it will correctly produce the correct translation, any external observer would be convinced the \"room\" speaks Chinese, while it is obvious, according to Searle, that neither the man nor the book actually understands Chinese. According to Searle, this thought experiment shows that a system can display the ability to solve a cognitive task without actually possessing the intelligence necessary to do it. It's supposed to show that imitation of intelligence is not the same as actual intelligence. Some have argued, though, that even if neither the man nor the book understands Chinese, there is a sense in which the whole system, the room with all its content, does understand Chinese. It is the same sense in which Turing's test defines intelligence: if the room (with man and book inside) is indeed able to provide a perfectly plausible translation for any incoming message, what else do we need to convince ourselves it does understand Chinese? The point is, this kind of argument rests on agreeing on the definition of what \"knowing\" or \"understanding\" means. If your definition of \"understanding\" is such that only humans are capable of doing it, then, by your own definition, you cannot believe in Artificial Intelligence. Turing was a functionalist in this regard. He believed that intelligence is best defined in terms of what it can do, regardless of, for example, how is it made or what is it composed of. In his view, intelligence is being capable of maintaining a coherent conversation about any general topic, independently of the hardware (electronic or biological) that sustains that intelligence. You might disagree, and lots of very smart people do. Some believe intelligence has to be biological in nature. Others, that there is some layer of unknown physics at play inside our brains that cannot be simulated in a classical computer. Turing believed the brain was just a very powerful computer, and thus, in principle, nothing can stop us from eventually making an electronic substitute capable of harbouring intelligence.","title":"The issue of semantics"},{"location":"podcast/what-is-ai/#the-issue-of-purpose","text":"Even if we agree philosophically with this definition of intelligence, it is still problematic in another sense. If we define intelligence as something at the level of what humans can currently do, isn't it kind of pointless to try and develop that? We already have humans, why do we need another system with the same capabilities? Wouldn't it be better if artificial intelligence can solve the problems that we humans cannot solve today? An argument in favour of Turing could be that his definition imposes no limitation to what this intelligence can do. It need not be as dumb as a human. In fact, a super-intelligent being should be perfectly capable of passing the test, the same way an adult can play with a child without actually believing in fairies. In a sense, we could say that being able to succeed at another human's judgement of character is the ultimate intelligence test because intelligence evolved precisely in this context: we were a bunch of primates in the savannah who needed to understand and trust each other to survive. But this anthropocentric view is not without issues as well. Who's to say that human intelligence is not a dead end? Couldn't we, by trying to imitate us, humans, be actually taking a detour on the path to superintelligence? Maybe our biases are just the evidence that, if there is higher intelligence in the universe, it is not like us. But if that's the case, could we even be able to recognize that intelligence? And could they, or it, recognize us? Turing's Test is far from perfect. It has practical, philosophical and ethical implications regarding how we define intelligence. In a sense, it is also about how we define ourselves since intelligence is arguable the human trait we are the proudest of. How we choose to define it also dictates how we attempt to imitate it, and how we go about recognizing it in others. Our biases are an intrinsic part of our intelligence, whatever that is.","title":"The issue of purpose"},{"location":"podcast/what-is-ai/#conclusions","text":"Turing's imitation game is the first attempt to formally define an overarching goal for artificial intelligence and the then nascent science of computation. However, this goal wasn't set in stone. In the next decades, different paradigms emerged, some of them more focused on what we today call general, or strong, AI, and others more focused on narrow, or weak, AI. The first group of paradigms attempts to solve AI more or less in the sense that Turing envisioned. To create an artificial intelligence so powerful that it can perform any cognitive task a human can, and then some we can't. The second attempts to solve concrete problems, such as vision, or language, or driving a car, that seem to require advanced levels of cognition but are not entirely general. For most of the history of AI, the narrow approaches have been the most successful. We are today capable of detecting tons of objects in images; translate between most mainstream languages; and play chess, Go, and even StarCraft better than any human that's ever lived. However, we are still very, very far from Turing's vision of achieving near-human capabilities at open-ended conversation. In a sense, we have walked a very long road, and yet we are still at the very beginning of this journey. And despite the dominating pragmatism, and the focus on solving narrow tasks, if you ask most of the people working today in the field, they will tell you that, deep down, they also share Turing's dream. Turing's life ended sadly. He was a gay man in a terribly unforgiving society, and he suffered physical and psychological humiliation for being unwilling to fit the narrow definition of human that others had decided. His greatest achievement was the dream he left us to pursue. A dream of a future in which humanity is no longer alone in the cognitive universe. A future we will share with our intellectual children, to whom we will show the marvels of the universe, and they will, in turn, help us unravel its deepest mysteries.","title":"Conclusions?"},{"location":"podcast/what-is-ai/#outro","text":"Thank you for listening. If you enjoyed this episode, feel free to leave a review, and share it with your loved ones. If you have any questions or suggestions, I would love to hear from you. You can find me on Twitter, and if you're listening on the Anchor app, you can leave a voice message right here. This was the Mostly Harmless AI podcast. I'll be back very soon with another episode on the fascinating world of Artificial Intelligence. Until then, stay curious, and stay safe...","title":"Outro"},{"location":"reading/","text":"Books I've read or I'm reading \u00b6 This is an ongoing list of the best books I've read or the ones I'm reading now. Some of them are especially recommended in some sense, others are just included here for completeness. If you want to know what I think about any of them, hit me up at Twitter . I'm including both fiction and non-fiction. I further separated some non-fiction books into a technical category. These are books whose main purpose is to teach you something, as opposed to, say, a discussion of some topic. Since the frontier is fuzzy, I made it based on my perceived intent of the author as well as on what I gained from the book.","title":"Books I've read or I'm reading"},{"location":"reading/#books-ive-read-or-im-reading","text":"This is an ongoing list of the best books I've read or the ones I'm reading now. Some of them are especially recommended in some sense, others are just included here for completeness. If you want to know what I think about any of them, hit me up at Twitter . I'm including both fiction and non-fiction. I further separated some non-fiction books into a technical category. These are books whose main purpose is to teach you something, as opposed to, say, a discussion of some topic. Since the frontier is fuzzy, I made it based on my perceived intent of the author as well as on what I gained from the book.","title":"Books I've read or I'm reading"},{"location":"reading/fiction/","text":"My favourite fiction books \u00b6 This is a list of all the fiction books I've read (or I'm currently reading) that I can remember of (if I cannot remember one, it didn't mean much for me). You'll see I read a lot of sci-fi, fantasy, and speculative fiction and in general. Not necessarily because I consider these genres any better than others, though. If I finished a book (and remember it) then I liked it; I rarely finish books I dislike. Now reading \u00b6 The Stand . Stephen King. In queue \u00b6 The Dark Tower . Stephen King. (books 3 through 7). My all-time favourites \u00b6 Dune . Frank Herbert. (the 6 original books). Foundation . Isaac Asimov. (the 7 original books). The Lord of the Rings . J.R.R. Tolkien. (3 books). The Dark Tower . Stephen King. (books 1 and 2). Stranger in a Strange Land . Good books for teenagers \u00b6 - Harry Potter . J.K.Rowling. \u00b6","title":"My favourite fiction books"},{"location":"reading/fiction/#my-favourite-fiction-books","text":"This is a list of all the fiction books I've read (or I'm currently reading) that I can remember of (if I cannot remember one, it didn't mean much for me). You'll see I read a lot of sci-fi, fantasy, and speculative fiction and in general. Not necessarily because I consider these genres any better than others, though. If I finished a book (and remember it) then I liked it; I rarely finish books I dislike.","title":"My favourite fiction books"},{"location":"reading/fiction/#now-reading","text":"The Stand . Stephen King.","title":"Now reading"},{"location":"reading/fiction/#in-queue","text":"The Dark Tower . Stephen King. (books 3 through 7).","title":"In queue"},{"location":"reading/fiction/#my-all-time-favourites","text":"Dune . Frank Herbert. (the 6 original books). Foundation . Isaac Asimov. (the 7 original books). The Lord of the Rings . J.R.R. Tolkien. (3 books). The Dark Tower . Stephen King. (books 1 and 2). Stranger in a Strange Land .","title":"My all-time favourites"},{"location":"reading/fiction/#good-books-for-teenagers","text":"","title":"Good books for teenagers"},{"location":"reading/fiction/#-harry-potter-jkrowling","text":"","title":"- Harry Potter. J.K.Rowling."},{"location":"reading/non-fiction/","text":"My favourite non-fiction books \u00b6 These are non-fiction books that I've enjoyed, either because I learned something or they make me change my mind somehow. I do not include here technical books, those are in a separate category, although that frontier is fuzzy. Currently reading \u00b6 A Sense of Style . Steven Pinker. History of Western Philosophy . Bertrand Russel. Paused \u00b6 Grit Thinking Fast and Slow . Daniel Kanheman. In queue \u00b6","title":"My favourite non-fiction books"},{"location":"reading/non-fiction/#my-favourite-non-fiction-books","text":"These are non-fiction books that I've enjoyed, either because I learned something or they make me change my mind somehow. I do not include here technical books, those are in a separate category, although that frontier is fuzzy.","title":"My favourite non-fiction books"},{"location":"reading/non-fiction/#currently-reading","text":"A Sense of Style . Steven Pinker. History of Western Philosophy . Bertrand Russel.","title":"Currently reading"},{"location":"reading/non-fiction/#paused","text":"Grit Thinking Fast and Slow . Daniel Kanheman.","title":"Paused"},{"location":"reading/non-fiction/#in-queue","text":"","title":"In queue"},{"location":"reading/technical/","text":"My favourite technical books \u00b6 These are technicall books that I've enjoyed, either because I learned something or they make me change my mind somehow. Currently reading \u00b6 Interpretable Machine Learning . Christoph Molnar.","title":"My favourite technical books"},{"location":"reading/technical/#my-favourite-technical-books","text":"These are technicall books that I've enjoyed, either because I learned something or they make me change my mind somehow.","title":"My favourite technical books"},{"location":"reading/technical/#currently-reading","text":"Interpretable Machine Learning . Christoph Molnar.","title":"Currently reading"}]}